{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Bike Rider Helmet Detection Abstract Bike rider helmet detection is a crucial computer vision task aimed at improving road safety by identifying bike rider who are not wearing helmets. This system leverages deep learning models, such as YOLOv8, to accurately detect helmets in real-time or from images or video streams. It can be integrated into traffic monitoring systems to automatically flag violations and enhance law enforcement efficiency. The implementation involves preprocessing images, training the model, and saving the model. I have also created a streamlit and django web application and deployed the web application in streamlit cloud. The proposed system helps reduce head injuries and fatalities in bike rider accidents by promoting helmet use. Index Sr. No Topic 1 1) Introduction 1.1) Computer Vision: Introduction 1.2) Bike Rider Helmet Detection: Introduction 2 2) Objective 2.1) Problem 2.2) Goal 3 3) Tools and Technology 3.1) Hardware Requirements 3.2) Software Requirements 4 4) Literature Review 4.1) Image Acquisition 4.2) Image Preprocessing 4.3) Feature Extraction 4.4) Object Detection and Recognition 4.5) Image Segmentation 4.6) Motion Analysis 4.7) Scene Understanding 5 5) Methodology 5.1) Data Collection 5.2) Data Pre-processing 5.3) Model Training 5.4) Model Evaluation 5.5) Model Testing 5.6) Deployment 6 6) Implementation Details 6.1) Dataset Used 6.2) YOLOv8 Metrics 6.3) What is YOLOv8? 6.4) Why YOLOv8? 6.5) What is Ultralytics? 6.6) What are Base Models? 6.7) What are Target Models? 6.8) How Pre-processing of the Dataset is Done? 6.9) How Images are Pre-processed? 6.10) How Videos are Pre-processed? 6.11) What is Ontology? 6.12) Sample Code 6.13) Data and YOLOv8 Configuration File 7 7) Analysis of Result 7.1) Confusion Matrix 7.2) Precision-Recall Curve and Train, Validation, and Loss Metrics 7.3) F1-Confidence and Recall-Confidence Curve 7.4) Precision-Recall Curve 7.5) Labels-Class Chart 7.6) Inference on Images 7.7) Inference on Videos 7.8) Django Web Application Demo 7.9) Streamlit Web Application Demo 7.10) Mkdocs Documentation Demo 8 8) Conclusion 9 9) Future Scope 10 10) Program Code 11 11) Reference Chapter 1 1.Introduction 1.1) Computer Vison: Introduction Computer Vision is an interdisciplinary field of study that enables computers to interpret and understand visual information from the world, much like the human visual system. It encompasses the development of algorithms, models, and systems that can process, analyze, and extract meaningful insights from visual data, typically in the form of images and videos. Computer Vision has wide-ranging applications across various industries, including healthcare, automotive, entertainment, surveillance, robotics, and more. This overview provides a comprehensive understanding of Computer Vision, its key components, applications, challenges, and future prospects. 1.2) Bike Rider Helmet Detection: Introduction The importance of road safety has been increasingly recognized, leading to the implementation of various measures to protect vulnerable road users such as cyclists and motorcyclists or bike riders. One critical safety measure is the use of helmets, which significantly reduces the risk of head injuries in the event of an accident. Despite regulations mandating helmet use, compliance remains a challenge. To address this, the Bike Rider Helmet Detection project aims to leverage advanced computer vision techniques to automatically detect helmet usage among bike riders and passangers. The Bike Rider Helmet Detection project utilizes one of the state-of-the-art deep learning models to accurately identify whether a bike rider is wearing a helmet. The core of the project is built on the YOLOv8 (You Only Look Once version 8) model, a highly efficient and powerful object detection algorithm developed and maintained by Ultralytics. YOLOv8 is renowned for its speed and accuracy, making it an ideal choice for real-time applications like helmet detection. There are a significant number of the accidents on the roads today. The accident can have multiple scenarios whether due to pot holes or any car accidentally bumbs bike driver while driving etc. By wearing a helmet not only we are saving our head from any fatal accidents but also saving any passanger life too. This explains why it is important to do more work in this field with an aim to reduce the occurrence of accidents related to any driving related injury and motivate overselves the importance of a helmet while riding a 2 wheeler vehicle. Chapter 2 2. Objective 2.1) Problem: Bike riders drivers who do not wear helmet which may result in fatal accidents and death in some cases. 2.2) Goal: Create a deep learning model that an detect if a person is wearing helmet or not. The main objective of this project is to develop a system that would detect if the bike rider along with the passengers are wearing a safety helmet or not. The dataset is collected through kaggle dataset. As manually annotation of the labels or semi label annotation would take significant amount of time. Only images are needed and and no explicit annotations are needed for this project as I am using ultralytics foundation models for auto labelling the images and video images to target model of YOLOv8. After training the model I have made a streamlit and django web application for this project along with documentation. Chapter 3 3. Tools and Technology As this is a deep learning project a significant amount of computation and memory allocation does matter a lot for this project. For inference of model on images and videos too required a significant amount of computation power. 3.1) Hardware Requirements :- 1) Desktop / Laptop / Server 2) 8 GB RAM at least 3) 150 GB Disk space or higher 4) Any processor Intel i5 / AMD 5) Google GPU - Tesla T4 3.2) Sofware Requirements :- 1) Windows / Ubuntu os (64 or 32 bit) 2) Google colab / Kaggle jupyter notebook 3) Python 3.10.12 or higher 4) Visual studio code editor, jupyter notebook 5) Sqlite version 0.5.6 or higher For more in depth requirements of the major, minor packages and respective project dependencies. Please take a look at the respective github repository. Chapter 4 4. Literature Review Key Components of Computer Vision: 4.1) Image Acquisition: Computer Vision begins with the acquisition of visual data, which can come from various sources, including cameras, sensors, or image databases. 4.2) Image Preprocessing: Raw visual data often requires preprocessing to enhance quality, remove noise, and prepare it for analysis. This includes tasks like image resizing, filtering, and color correction. 4.3) Feature Extraction: Feature extraction involves identifying and isolating relevant visual patterns or features within an image, such as edges, corners, or texture. 4.4) Object Detection and Recognition: This component focuses on identifying and classifying objects within images or videos. Object detection and recognition algorithms enable computers to recognize and label objects, faces, or specific patterns. 4.5) Image Segmentation: Image segmentation divides an image into meaningful regions or segments. It's crucial for tasks like medical image analysis, where different parts of an image may represent different anatomical structures. 4.6) Motion Analysis: Motion analysis techniques track moving objects and can be used in applications like surveillance, sports analysis, and robotics. 4.7) Scene Understanding: This involves higher-level interpretation of images or videos to understand the context and relationships between objects within a scene. Chapter 5 5. Methodology 5.1) Data Collection: Collect publicly available images / videos of bike rider, helmet and no helmet for the model to train upon. The images are collected from kaggle dataset 5.2) Data pre-processing: Pre-processing and auto labeling images and videos are done by ultralytics framework by using a Grounding Segment Anything Model based on Ontology. For videos every 3 frame is considered. 5.3) Model Training: Train YOLOv8 Model on custom dataset with 3 classes, Socastic Gradient Decent (SGD) optimizer, learning rate 0.01, batch size 16, epochs 50, image size 640, momentum 0.937, validation split around 20 %. 5.4) Model Evaluation: Evaluate trained YOLOv8 Model on validation dataset with the metrics such as box loss, class loss, precision and recall. Confusion matrix. 5.5) Model Testing: Run Inference on images and videos. To check how well the model perform. 5.6) Deployment: The final model is deployed as a streamlit web application and django for local deployment along with application programming interface (API). Chapter 6 6. Implementation details 6.1) Dataset Used: The dataset that is used for this project can be find below:- https://www.kaggle.com/datasets/andrewmvd/helmet-detection This dataset contains 764 images of 2 distinct classes for the objective of helmet detection. Bounding box annotations are provided in the PASCAL VOC format. The classes are: - With helmet - Without helmet 6.2) YOLOv8 Metrics: 6.3) What is YOLOv8? YOLOv8 is from the YOLO family of models and was released on January 10, 2023. YOLO stands for You Only Look Once, and this series of models are thus named because of their ability to predict every object present in an image with one forward pass. 6.4) Why YOLOv8? YOLOv8 by ultalytics is a state-of-the-art deep learning model designed for real- time object detection in computer vision applications. With its advanced architecture and cutting-edge algorithms, YOLOv8 has revolutionized the field of object detection, enabling accurate and efficient detection of objects in real-time scenarios. YOLOv8 is quite stable as compare to latest YOLOv9 and recent YOLOv10. 6.5) What is Ultralytics? Ultralytics is a company that specializes in developing advanced computer vision technologies, particularly the YOLO (You Only Look Once) series of models. Their framework, Ultralytics YOLO, provides state-of-the-art object detection, image segmentation, and classification capabilities. The base model, often a pre-trained YOLOv8 model or it can be any model like YOLOv5, can be fine-tuned or used directly to create custom target models for various applications for any custom dataset. This framework is widely used for real-time object detection tasks due to its speed and accuracy. 6.6) What are base models? Base Model - A Base Model is a large foundation model that knows a lot about a lot. Base models are often multimodal and can perform many tasks. They're large, slow, and expensive. Examples of Base Models are GroundedSAM and GPT-4's upcoming multimodal variant. We use a Base Model (along with unlabeled input data and an Ontology) to create a Dataset. 6.7) What are target models? Target Model - a Target Model is a supervised model that consumes a dataset and outputs a distilled model that is ready for deployment. Target Models are usually small, fast, and fine-tuned to perform a specific task very well (but they don't generalize well beyond the information described in their Dataset). Examples of Target Models are YOLOv8 and YOLOv5. 6.8) How pre-processing of the dataset is done? Only images and videos are needed for this project and no annotations are needed from the data as annotation are generated by ultralytics framework called as Autodistill which uses Grounding SAM which is combination of Grounding DiNO (Zero short object detection model) and SAM (Segment Anything Model) (zero short object detection with prompting) from Meta for autolabeling dataset and preprocess and train on YOLOv8 large model. Autodistill uses big, slower foundation models to train small, faster supervised models. Using autodistill, you can go from unlabeled images to inference on a custom model running at the edge with no human intervention in between. As foundation models get better and better they will increasingly be able to augment or replace humans in the labeling process. 6.9) How images are pre-processed? The dataset image consist of 764 images of various Bike rider, Rider wearing helmet, Rider not wearing helmet. Out of 764 images only 611 images are used. I have removed some images because my google colab kernel is crashing significantly. 6.10) How videos are pre-processed? Video is processed is such a way in which every 3 frame (can be changed through code) are considered as image data for the model to train upon. For autolabelling the dataset of images and video frame autodistill uses something which is called as ontology 6.11) What is Ontology? Ontology - an Ontology defines how your Base Model like Grounding SAM is prompted, what your Dataset will describe, and what your Target Model like YOLOv8 will predict. A simple Ontology is the CaptionOntology which prompts a Base Model with text captions and maps them to class names. Other Ontologies may, for instance, use a CLIP vector or example images instead of a text caption. from autodistill.detection import CaptionOntology # \"<description of label>\": \"<label_name>\" # \"bike rider\": \"Bike_Rider\", --> label 0 # \"bike rider and passanger with helmet\": \"Helmet\", --> label 1 # \"bike rider and passanger with no helmet\": \"No_Helmet\" --> label 2 ontology=CaptionOntology({ \"bike rider\": \"Bike_Rider\", \"helmet\": \"Helmet\", \"no helmet\": \"No_Helmet\" }) The preprocessed dataset is then divided into training and validation dataset to check the performance of model. This YOLOv8 model is fine-tuned for custom dataset target values of bike rider , helmet and no helmet . Based on which best.pt and last.pt pytorch training weights are generated. 6.12) Sample Code 6.13) Data and YOLOv8 configuration file Chapter 7 7. Analysis of result 7.1) Confusion Matrix 7.2) Precision-Confidence Curve and Train, Validation and Loss metrics 7.3) F1-Confidence and Recall-Confidence Curve 7.4) Precision-Recall Curve 7.5) Labels-Class Chart 7.6) Inference on Images Inference on Video 7.8) Django Web application Demo Index Page Django admin panel Sqlite3 database API end points 7.9) Streamlit Web application Demo Drag and drop the image for object detections Select the video and click Detect Video Objects button Works on only web camera Please make sure web camera is connected Works on native device camera (Webcam, Smartphone) Select respective device and click on start button Insert youtube url and click on Detect Objects button 7.10) Mkdocs Documentation Demo Chapter 8 8. Conclusion In conclusion, the development of a computer vision model using YOLOv8 for bike rider helmet detection represents a significant advancement in enhancing safety measures for riders along with passengers. The model demonstrates impressive accuracy and efficiency in identifying helmet-wearing individuals, which is crucial for ensuring compliance with safety regulations and reducing the risk of head injuries in bike-related accidents. This project highlights the potential of computer vision technology to address real- world safety challenges effectively. Chapter 9 9. Future Scope The Future scope of this project should be focused on latest implementation of YOLOv9 and recent YOLOv10. There should be seperate repository for implementation of YOLOv9 and YOLOv10 along with any demo related to it with any technology for creating web application like django, flask, mesop, streamlit, gradio etc. or any desktop application using various techology as reflex, nicegui, tkinter, kivy etc. Chapter 10 10. Program code 1) Implementation of Bike Helmet Detection Jupyter Notebook https://github.com/Viddesh1/Helmet_test_1 2) Output generated by YOLOv8 https://drive.google.com/drive/folders/ 1M4FckJJeyPQTTWqgo6xWhW8L4tf0EJ4l?usp=sharing 3) Bike Helmet Detection Django Web application https://github.com/Viddesh1/Bike-Helmet-Detection 4) Bike Helmet Detection Stremlit Web application https://github.com/Viddesh1/Bike-Helmet-Detectionv2 5) Hosted on Streamlit https://bike-helmet-detectionv2-dmehozp3lkef4wnssaepjf.streamlit.app/ 6) Overall Documentation https://github.com/Viddesh1/Bike-Helmet-Detection-Docs Chapter 11 11. References 1) https://github.com/ultralytics/ultralytics 2) https://towardsdatascience.com/dino-emerging-properties-in-self-supervised-vision- transformers-summary-ab91df82cc3c 3) https://github.com/facebookresearch/dino/ 4) Emerging Properties in Self-Supervised Vision Transformers : https://arxiv.org/abs/2104.14294/ 5) https://dinov2.metademolab.com/ 6) https://segment-anything.com/ 7) https://blog.roboflow.com/whats-new-in-yolov8/","title":"Index"},{"location":"#bike-rider-helmet-detection","text":"","title":"Bike Rider Helmet Detection"},{"location":"#abstract","text":"Bike rider helmet detection is a crucial computer vision task aimed at improving road safety by identifying bike rider who are not wearing helmets. This system leverages deep learning models, such as YOLOv8, to accurately detect helmets in real-time or from images or video streams. It can be integrated into traffic monitoring systems to automatically flag violations and enhance law enforcement efficiency. The implementation involves preprocessing images, training the model, and saving the model. I have also created a streamlit and django web application and deployed the web application in streamlit cloud. The proposed system helps reduce head injuries and fatalities in bike rider accidents by promoting helmet use.","title":"Abstract"},{"location":"#index","text":"Sr. No Topic 1 1) Introduction 1.1) Computer Vision: Introduction 1.2) Bike Rider Helmet Detection: Introduction 2 2) Objective 2.1) Problem 2.2) Goal 3 3) Tools and Technology 3.1) Hardware Requirements 3.2) Software Requirements 4 4) Literature Review 4.1) Image Acquisition 4.2) Image Preprocessing 4.3) Feature Extraction 4.4) Object Detection and Recognition 4.5) Image Segmentation 4.6) Motion Analysis 4.7) Scene Understanding 5 5) Methodology 5.1) Data Collection 5.2) Data Pre-processing 5.3) Model Training 5.4) Model Evaluation 5.5) Model Testing 5.6) Deployment 6 6) Implementation Details 6.1) Dataset Used 6.2) YOLOv8 Metrics 6.3) What is YOLOv8? 6.4) Why YOLOv8? 6.5) What is Ultralytics? 6.6) What are Base Models? 6.7) What are Target Models? 6.8) How Pre-processing of the Dataset is Done? 6.9) How Images are Pre-processed? 6.10) How Videos are Pre-processed? 6.11) What is Ontology? 6.12) Sample Code 6.13) Data and YOLOv8 Configuration File 7 7) Analysis of Result 7.1) Confusion Matrix 7.2) Precision-Recall Curve and Train, Validation, and Loss Metrics 7.3) F1-Confidence and Recall-Confidence Curve 7.4) Precision-Recall Curve 7.5) Labels-Class Chart 7.6) Inference on Images 7.7) Inference on Videos 7.8) Django Web Application Demo 7.9) Streamlit Web Application Demo 7.10) Mkdocs Documentation Demo 8 8) Conclusion 9 9) Future Scope 10 10) Program Code 11 11) Reference","title":"Index"},{"location":"#chapter-1","text":"","title":"Chapter 1"},{"location":"#1introduction","text":"1.1) Computer Vison: Introduction Computer Vision is an interdisciplinary field of study that enables computers to interpret and understand visual information from the world, much like the human visual system. It encompasses the development of algorithms, models, and systems that can process, analyze, and extract meaningful insights from visual data, typically in the form of images and videos. Computer Vision has wide-ranging applications across various industries, including healthcare, automotive, entertainment, surveillance, robotics, and more. This overview provides a comprehensive understanding of Computer Vision, its key components, applications, challenges, and future prospects. 1.2) Bike Rider Helmet Detection: Introduction The importance of road safety has been increasingly recognized, leading to the implementation of various measures to protect vulnerable road users such as cyclists and motorcyclists or bike riders. One critical safety measure is the use of helmets, which significantly reduces the risk of head injuries in the event of an accident. Despite regulations mandating helmet use, compliance remains a challenge. To address this, the Bike Rider Helmet Detection project aims to leverage advanced computer vision techniques to automatically detect helmet usage among bike riders and passangers. The Bike Rider Helmet Detection project utilizes one of the state-of-the-art deep learning models to accurately identify whether a bike rider is wearing a helmet. The core of the project is built on the YOLOv8 (You Only Look Once version 8) model, a highly efficient and powerful object detection algorithm developed and maintained by Ultralytics. YOLOv8 is renowned for its speed and accuracy, making it an ideal choice for real-time applications like helmet detection. There are a significant number of the accidents on the roads today. The accident can have multiple scenarios whether due to pot holes or any car accidentally bumbs bike driver while driving etc. By wearing a helmet not only we are saving our head from any fatal accidents but also saving any passanger life too. This explains why it is important to do more work in this field with an aim to reduce the occurrence of accidents related to any driving related injury and motivate overselves the importance of a helmet while riding a 2 wheeler vehicle.","title":"1.Introduction"},{"location":"#chapter-2","text":"","title":"Chapter 2"},{"location":"#2-objective","text":"2.1) Problem: Bike riders drivers who do not wear helmet which may result in fatal accidents and death in some cases. 2.2) Goal: Create a deep learning model that an detect if a person is wearing helmet or not. The main objective of this project is to develop a system that would detect if the bike rider along with the passengers are wearing a safety helmet or not. The dataset is collected through kaggle dataset. As manually annotation of the labels or semi label annotation would take significant amount of time. Only images are needed and and no explicit annotations are needed for this project as I am using ultralytics foundation models for auto labelling the images and video images to target model of YOLOv8. After training the model I have made a streamlit and django web application for this project along with documentation.","title":"2. Objective"},{"location":"#chapter-3","text":"","title":"Chapter 3"},{"location":"#3-tools-and-technology","text":"As this is a deep learning project a significant amount of computation and memory allocation does matter a lot for this project. For inference of model on images and videos too required a significant amount of computation power. 3.1) Hardware Requirements :- 1) Desktop / Laptop / Server 2) 8 GB RAM at least 3) 150 GB Disk space or higher 4) Any processor Intel i5 / AMD 5) Google GPU - Tesla T4 3.2) Sofware Requirements :- 1) Windows / Ubuntu os (64 or 32 bit) 2) Google colab / Kaggle jupyter notebook 3) Python 3.10.12 or higher 4) Visual studio code editor, jupyter notebook 5) Sqlite version 0.5.6 or higher For more in depth requirements of the major, minor packages and respective project dependencies. Please take a look at the respective github repository.","title":"3. Tools and Technology"},{"location":"#chapter-4","text":"","title":"Chapter 4"},{"location":"#4-literature-review","text":"Key Components of Computer Vision: 4.1) Image Acquisition: Computer Vision begins with the acquisition of visual data, which can come from various sources, including cameras, sensors, or image databases. 4.2) Image Preprocessing: Raw visual data often requires preprocessing to enhance quality, remove noise, and prepare it for analysis. This includes tasks like image resizing, filtering, and color correction. 4.3) Feature Extraction: Feature extraction involves identifying and isolating relevant visual patterns or features within an image, such as edges, corners, or texture. 4.4) Object Detection and Recognition: This component focuses on identifying and classifying objects within images or videos. Object detection and recognition algorithms enable computers to recognize and label objects, faces, or specific patterns. 4.5) Image Segmentation: Image segmentation divides an image into meaningful regions or segments. It's crucial for tasks like medical image analysis, where different parts of an image may represent different anatomical structures. 4.6) Motion Analysis: Motion analysis techniques track moving objects and can be used in applications like surveillance, sports analysis, and robotics. 4.7) Scene Understanding: This involves higher-level interpretation of images or videos to understand the context and relationships between objects within a scene.","title":"4. Literature Review"},{"location":"#chapter-5","text":"","title":"Chapter 5"},{"location":"#5-methodology","text":"5.1) Data Collection: Collect publicly available images / videos of bike rider, helmet and no helmet for the model to train upon. The images are collected from kaggle dataset 5.2) Data pre-processing: Pre-processing and auto labeling images and videos are done by ultralytics framework by using a Grounding Segment Anything Model based on Ontology. For videos every 3 frame is considered. 5.3) Model Training: Train YOLOv8 Model on custom dataset with 3 classes, Socastic Gradient Decent (SGD) optimizer, learning rate 0.01, batch size 16, epochs 50, image size 640, momentum 0.937, validation split around 20 %. 5.4) Model Evaluation: Evaluate trained YOLOv8 Model on validation dataset with the metrics such as box loss, class loss, precision and recall. Confusion matrix. 5.5) Model Testing: Run Inference on images and videos. To check how well the model perform. 5.6) Deployment: The final model is deployed as a streamlit web application and django for local deployment along with application programming interface (API).","title":"5. Methodology"},{"location":"#chapter-6","text":"","title":"Chapter 6"},{"location":"#6-implementation-details","text":"6.1) Dataset Used: The dataset that is used for this project can be find below:- https://www.kaggle.com/datasets/andrewmvd/helmet-detection This dataset contains 764 images of 2 distinct classes for the objective of helmet detection. Bounding box annotations are provided in the PASCAL VOC format. The classes are: - With helmet - Without helmet 6.2) YOLOv8 Metrics: 6.3) What is YOLOv8? YOLOv8 is from the YOLO family of models and was released on January 10, 2023. YOLO stands for You Only Look Once, and this series of models are thus named because of their ability to predict every object present in an image with one forward pass. 6.4) Why YOLOv8? YOLOv8 by ultalytics is a state-of-the-art deep learning model designed for real- time object detection in computer vision applications. With its advanced architecture and cutting-edge algorithms, YOLOv8 has revolutionized the field of object detection, enabling accurate and efficient detection of objects in real-time scenarios. YOLOv8 is quite stable as compare to latest YOLOv9 and recent YOLOv10. 6.5) What is Ultralytics? Ultralytics is a company that specializes in developing advanced computer vision technologies, particularly the YOLO (You Only Look Once) series of models. Their framework, Ultralytics YOLO, provides state-of-the-art object detection, image segmentation, and classification capabilities. The base model, often a pre-trained YOLOv8 model or it can be any model like YOLOv5, can be fine-tuned or used directly to create custom target models for various applications for any custom dataset. This framework is widely used for real-time object detection tasks due to its speed and accuracy. 6.6) What are base models? Base Model - A Base Model is a large foundation model that knows a lot about a lot. Base models are often multimodal and can perform many tasks. They're large, slow, and expensive. Examples of Base Models are GroundedSAM and GPT-4's upcoming multimodal variant. We use a Base Model (along with unlabeled input data and an Ontology) to create a Dataset. 6.7) What are target models? Target Model - a Target Model is a supervised model that consumes a dataset and outputs a distilled model that is ready for deployment. Target Models are usually small, fast, and fine-tuned to perform a specific task very well (but they don't generalize well beyond the information described in their Dataset). Examples of Target Models are YOLOv8 and YOLOv5. 6.8) How pre-processing of the dataset is done? Only images and videos are needed for this project and no annotations are needed from the data as annotation are generated by ultralytics framework called as Autodistill which uses Grounding SAM which is combination of Grounding DiNO (Zero short object detection model) and SAM (Segment Anything Model) (zero short object detection with prompting) from Meta for autolabeling dataset and preprocess and train on YOLOv8 large model. Autodistill uses big, slower foundation models to train small, faster supervised models. Using autodistill, you can go from unlabeled images to inference on a custom model running at the edge with no human intervention in between. As foundation models get better and better they will increasingly be able to augment or replace humans in the labeling process. 6.9) How images are pre-processed? The dataset image consist of 764 images of various Bike rider, Rider wearing helmet, Rider not wearing helmet. Out of 764 images only 611 images are used. I have removed some images because my google colab kernel is crashing significantly. 6.10) How videos are pre-processed? Video is processed is such a way in which every 3 frame (can be changed through code) are considered as image data for the model to train upon. For autolabelling the dataset of images and video frame autodistill uses something which is called as ontology 6.11) What is Ontology? Ontology - an Ontology defines how your Base Model like Grounding SAM is prompted, what your Dataset will describe, and what your Target Model like YOLOv8 will predict. A simple Ontology is the CaptionOntology which prompts a Base Model with text captions and maps them to class names. Other Ontologies may, for instance, use a CLIP vector or example images instead of a text caption. from autodistill.detection import CaptionOntology # \"<description of label>\": \"<label_name>\" # \"bike rider\": \"Bike_Rider\", --> label 0 # \"bike rider and passanger with helmet\": \"Helmet\", --> label 1 # \"bike rider and passanger with no helmet\": \"No_Helmet\" --> label 2 ontology=CaptionOntology({ \"bike rider\": \"Bike_Rider\", \"helmet\": \"Helmet\", \"no helmet\": \"No_Helmet\" }) The preprocessed dataset is then divided into training and validation dataset to check the performance of model. This YOLOv8 model is fine-tuned for custom dataset target values of bike rider , helmet and no helmet . Based on which best.pt and last.pt pytorch training weights are generated. 6.12) Sample Code 6.13) Data and YOLOv8 configuration file","title":"6. Implementation details"},{"location":"#chapter-7","text":"","title":"Chapter 7"},{"location":"#7-analysis-of-result","text":"7.1) Confusion Matrix 7.2) Precision-Confidence Curve and Train, Validation and Loss metrics 7.3) F1-Confidence and Recall-Confidence Curve 7.4) Precision-Recall Curve 7.5) Labels-Class Chart 7.6) Inference on Images","title":"7. Analysis of result"},{"location":"#inference-on-video","text":"7.8) Django Web application Demo","title":"Inference on Video"},{"location":"#index-page","text":"","title":"Index Page"},{"location":"#django-admin-panel","text":"","title":"Django admin panel"},{"location":"#sqlite3-database","text":"","title":"Sqlite3 database"},{"location":"#api-end-points","text":"7.9) Streamlit Web application Demo","title":"API end points"},{"location":"#drag-and-drop-the-image-for-object-detections","text":"","title":"Drag and drop the image for object detections"},{"location":"#select-the-video-and-click-detect-video-objects-button","text":"","title":"Select the video and click Detect Video Objects button"},{"location":"#works-on-only-web-camera","text":"Please make sure web camera is connected","title":"Works on only web camera"},{"location":"#works-on-native-device-camera-webcam-smartphone","text":"Select respective device and click on start button","title":"Works on native device camera (Webcam, Smartphone)"},{"location":"#insert-youtube-url-and-click-on-detect-objects-button","text":"7.10) Mkdocs Documentation Demo","title":"Insert youtube url and click on Detect Objects button"},{"location":"#chapter-8","text":"","title":"Chapter 8"},{"location":"#8-conclusion","text":"In conclusion, the development of a computer vision model using YOLOv8 for bike rider helmet detection represents a significant advancement in enhancing safety measures for riders along with passengers. The model demonstrates impressive accuracy and efficiency in identifying helmet-wearing individuals, which is crucial for ensuring compliance with safety regulations and reducing the risk of head injuries in bike-related accidents. This project highlights the potential of computer vision technology to address real- world safety challenges effectively.","title":"8. Conclusion"},{"location":"#chapter-9","text":"","title":"Chapter 9"},{"location":"#9-future-scope","text":"The Future scope of this project should be focused on latest implementation of YOLOv9 and recent YOLOv10. There should be seperate repository for implementation of YOLOv9 and YOLOv10 along with any demo related to it with any technology for creating web application like django, flask, mesop, streamlit, gradio etc. or any desktop application using various techology as reflex, nicegui, tkinter, kivy etc.","title":"9. Future Scope"},{"location":"#chapter-10","text":"","title":"Chapter 10"},{"location":"#10-program-code","text":"1) Implementation of Bike Helmet Detection Jupyter Notebook https://github.com/Viddesh1/Helmet_test_1 2) Output generated by YOLOv8 https://drive.google.com/drive/folders/ 1M4FckJJeyPQTTWqgo6xWhW8L4tf0EJ4l?usp=sharing 3) Bike Helmet Detection Django Web application https://github.com/Viddesh1/Bike-Helmet-Detection 4) Bike Helmet Detection Stremlit Web application https://github.com/Viddesh1/Bike-Helmet-Detectionv2 5) Hosted on Streamlit https://bike-helmet-detectionv2-dmehozp3lkef4wnssaepjf.streamlit.app/ 6) Overall Documentation https://github.com/Viddesh1/Bike-Helmet-Detection-Docs","title":"10. Program code"},{"location":"#chapter-11","text":"","title":"Chapter 11"},{"location":"#11-references","text":"1) https://github.com/ultralytics/ultralytics 2) https://towardsdatascience.com/dino-emerging-properties-in-self-supervised-vision- transformers-summary-ab91df82cc3c 3) https://github.com/facebookresearch/dino/ 4) Emerging Properties in Self-Supervised Vision Transformers : https://arxiv.org/abs/2104.14294/ 5) https://dinov2.metademolab.com/ 6) https://segment-anything.com/ 7) https://blog.roboflow.com/whats-new-in-yolov8/","title":"11. References"},{"location":"bhd/","text":"Bike Rider Helmet Detection using YOLOv8 This repository contains a walk through of implementation of bike rider helmet detection using YOLOv8. As we know that bike riders who do not wear helmet may which result in fatal accidents and death in some cases. The goal is to create a ML/DL model that can detect if a person is wearing helment or not. May sure to select GPU in google colab or kaggle notebook for running through the notebook. As running through the code in local machine with no graphical processing unit will take a very significant ammount of time. Methodology 1) Data Collection:- Collect images/videos of bike rider, helmet and no helmet for the model to train upon. 2) Data pre-processing:- Pre-processing and autolabel images and videos using foundation model like DINO and SAM (Segment anything model). 3) Train YOLOv8 Model. 4) Evaluate Target Model. 5) Run Inference on images and videos. How to setup codebase locally? python3 -m venv .venv # Create a virtual environment source .venv/bin/activate # Activate a virtual environment git clone https://github.com/Viddesh1/Helmet_test_1.git cd Helmet_test_1/ Repository File Structure Helmet_test_1 # This repository root folder \u251c\u2500\u2500 .git # For managing files by git \u251c\u2500\u2500 .gitignore # Files to be not managed by git \u251c\u2500\u2500 Helmet_how_to_auto_train_yolov8_model_with_autodistill.ipynb # For reference \u251c\u2500\u2500 kaggle_Helmet_how_to_auto_train_yolov8_model_with_autodistill.ipynb # YOLOv8 implementation \u251c\u2500\u2500 README.md # This README.md file itself \u2514\u2500\u2500 tree_all.txt # This file structure from tree -a > tree_all.txt command Dataset Information For this project only images and videos are needed and no annotations files are needed. As for this project auto labelling is done by using the Ultralytics framework which uses base model naming Grounded SAM (Segment Anything Model) which is a combination of Grounding DINO (Deeper Into Neural Networks) + SAM (Segment Anything model). The target model is YOLOv8. The datset that is used for this project can be find below:- https://www.kaggle.com/datasets/andrewmvd/helmet-detection This dataset contains 764 images of 2 distinct classes for the objective of helmet detection. Bounding box annotations are provided in the PASCAL VOC format. The classes are: With helmet Without helmet Please generate your own kaggle api key for accessing the dataset with in google colab or kaggle notebook Please take a look at the below python code representing labels for individual ontology. from autodistill.detection import CaptionOntology # \"<description of label>\": \"<label_name>\" # \"bike rider\": \"Bike_Rider\", --> label 0 # \"bike rider and passanger with helmet\": \"Helmet\", --> label 1 # \"bike rider and passanger with no helmet\": \"No_Helmet\" --> label 2 ontology=CaptionOntology({ \"bike rider\": \"Bike_Rider\", \"helmet\": \"Helmet\", \"no helmet\": \"No_Helmet\" }) Dataset File Structure archive # Root file directory of https://www.kaggle.com/datasets/andrewmvd/helmet-detection \u251c\u2500\u2500 annotations # Annotations based on PASCAL VOC format as XML files \u2502 \u251c\u2500\u2500 BikesHelmets0.xml \u2502 \u251c\u2500\u2500 BikesHelmets100.xml \u2502 \u251c\u2500\u2500 BikesHelmets101.xml \u251c\u2500\u2500 images # Public images for classes helmet and without helmet images \u2502 \u251c\u2500\u2500 BikesHelmets0.png \u2502 \u251c\u2500\u2500 BikesHelmets100.png \u2502 \u251c\u2500\u2500 BikesHelmets101.png \u2514\u2500\u2500 tree_all.txt 2 directories, 1529 files This repository output may change in the near future:- https://drive.google.com/drive/folders/1M4FckJJeyPQTTWqgo6xWhW8L4tf0EJ4l?usp=sharing Output File Structure for YOLOv8 YOLOv8_Helmet_V0 # Main directory of YOLOv8 output \u251c\u2500\u2500 dataset \u2502 \u251c\u2500\u2500 annotations # Empty file \u2502 \u251c\u2500\u2500 data.yaml # Information related to data detection labels and train, valid path \u2502 \u251c\u2500\u2500 images # Empty folder \u2502 \u251c\u2500\u2500 train # Train dataset \u2502 \u2502 \u251c\u2500\u2500 images # Images directory path \u2502 \u2502 \u2502 \u251c\u2500\u2500 BikesHelmets0.jpg \u2502 \u2502 \u2502 \u251c\u2500\u2500 BikesHelmets100.jpg \u2502 \u2502 \u2502 \u251c\u2500\u2500 BikesHelmets101.jpg \u2502 \u2502 \u251c\u2500\u2500 labels # Annotations directory path \u2502 \u2502 \u2502 \u251c\u2500\u2500 BikesHelmets0.txt \u2502 \u2502 \u2502 \u251c\u2500\u2500 BikesHelmets100.txt \u2502 \u2502 \u2502 \u251c\u2500\u2500 BikesHelmets101.txt \u2502 \u2502 \u2514\u2500\u2500 labels.cache \u2502 \u2514\u2500\u2500 valid # Validation dataset for testing trained model \u2502 \u251c\u2500\u2500 images # Validation dataset images \u2502 \u2502 \u251c\u2500\u2500 BikesHelmets103.jpg \u2502 \u2502 \u251c\u2500\u2500 BikesHelmets108.jpg \u2502 \u2502 \u251c\u2500\u2500 BikesHelmets119.jpg \u2502 \u251c\u2500\u2500 labels # Validation dataset labels \u2502 \u2502 \u251c\u2500\u2500 BikesHelmets103.txt \u2502 \u2502 \u251c\u2500\u2500 BikesHelmets108.txt \u2502 \u2502 \u251c\u2500\u2500 BikesHelmets119.txt \u2502 \u2514\u2500\u2500 labels.cache \u251c\u2500\u2500 images # All 764 images form the dataset \u2502 \u251c\u2500\u2500 BikesHelmets0.png \u2502 \u251c\u2500\u2500 BikesHelmets100.png \u2502 \u251c\u2500\u2500 BikesHelmets101.png \u251c\u2500\u2500 runs # Predictions \u2502 \u2514\u2500\u2500 detect \u2502 \u251c\u2500\u2500 predict \u2502 \u2502 \u2514\u2500\u2500 he2.mp4 # Inference upon video 1 \u2502 \u251c\u2500\u2500 predict2 \u2502 \u2502 \u2514\u2500\u2500 test_1.mp4 # Inference upon video 2 \u2502 \u251c\u2500\u2500 predict3 \u2502 \u2502 \u2514\u2500\u2500 test_2.mp4 # Inference upon video 3 \u2502 \u2514\u2500\u2500 train \u2502 \u251c\u2500\u2500 args.yaml # Configuration blue print for training the YOLOv8 model parameters \u2502 \u251c\u2500\u2500 confusion_matrix.png \u2502 \u251c\u2500\u2500 events.out.tfevents.1697046331.428f98cba7b3.163.0 \u2502 \u251c\u2500\u2500 F1_curve.png \u2502 \u251c\u2500\u2500 labels_correlogram.jpg \u2502 \u251c\u2500\u2500 labels.jpg \u2502 \u251c\u2500\u2500 P_curve.png \u2502 \u251c\u2500\u2500 PR_curve.png \u2502 \u251c\u2500\u2500 R_curve.png \u2502 \u251c\u2500\u2500 results.csv # All the metrics for train_loss, class_loss, lr etc \u2502 \u251c\u2500\u2500 results.png # Generated from results.csv \u2502 \u251c\u2500\u2500 train_batch0.jpg \u2502 \u251c\u2500\u2500 train_batch1.jpg \u2502 \u251c\u2500\u2500 train_batch2.jpg \u2502 \u251c\u2500\u2500 val_batch0_labels.jpg \u2502 \u251c\u2500\u2500 val_batch0_pred.jpg \u2502 \u251c\u2500\u2500 val_batch1_labels.jpg \u2502 \u251c\u2500\u2500 val_batch1_pred.jpg \u2502 \u251c\u2500\u2500 val_batch2_labels.jpg \u2502 \u251c\u2500\u2500 val_batch2_pred.jpg \u2502 \u2514\u2500\u2500 weights # Model weights after training on custom dataset \u2502 \u251c\u2500\u2500 best.pt # Best model as pytorch format \u2502 \u2514\u2500\u2500 last.pt # Last model as pytorch format \u251c\u2500\u2500 tree_all.txt # Generated by tree -a > tree_all.txt \u251c\u2500\u2500 videos # Videos to be uploaded for preprocessing \u2502 \u251c\u2500\u2500 he2.mp4 \u2502 \u251c\u2500\u2500 test_1.mp4 \u2502 \u2514\u2500\u2500 test_2.mp4 \u251c\u2500\u2500 yolov8l.pt # Default YOLOV8 large model \u2514\u2500\u2500 yolov8n.pt # Default YOLOv8 nano model External packages needed for this project !pip install -q kaggle # For accessing dataset locally in google colab !pip install -U ultralytics !pip install -q \\ # install require packages in quite mode autodistill \\ # Automates model distillation autodistill-grounded-sam \\ # Enhanced distillation with grounding and self-attention mechanisms. autodistill-yolov8 \\ # distilling YOLOv8 models supervision==0.9.0 # For supervising models Train target model - YOLOv8 %cd {HOME} from autodistill_yolov8 import YOLOv8 target_model = YOLOv8(\"yolov8l.pt\") target_model.train(DATA_YAML_PATH, epochs=50) #100 Relevant Information of target model YOLOv8 Model Download : File : yolov8l.pt Source : Ultralytics GitHub repository :- (https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8l.pt) Description : Downloads the pre-trained YOLOv8 large model, which is approximately 83.7 MB in size. The model is used for object detection tasks. YOLO library : Current Version : 8.0.81 New Version Available : 8.0.196 Update Command : pip install -U ultralytics Description : Suggests updating the Ultralytics library to the latest version for improved features and performance. Environment Details : Python Version : 3.10.12 PyTorch Version : 2.0.1+cu118 CUDA Version : 0 (CUDA enabled device: Tesla T4 with 15102 MiB memory) Description : Specifies the software and hardware environment used for running the YOLOv8 model. Training Configuration : Task : detect Mode : train Model : yolov8l.pt Data : /content/dataset/data.yaml Epochs : 50 Patience : 50 Batch Size : 16 Image Size : 640 Optimizer : SGD Other Parameters : Various other hyperparameters and settings for training the model. Description : Outlines the configuration for training the YOLOv8 model using the specified dataset, model, and hyperparameters. Font Download : File : Arial.ttf Source : Ultralytics website (https://ultralytics.com/assets/Arial.ttf) Location : /root/.config/Ultralytics/Arial.ttf Description : Downloads a font file required for visualizations and plots generated during the training process. Model Configuration Override : Original nc : 80 New nc : 3 Description : Overrides the number of classes ( nc ) in the model configuration to 3, as specified in the dataset configuration ( data.yaml ). Optimizer Type : SGD Learning Rate : 0.01 Parameter Groups : Group 1: Weight Decay: 0.0 Number of Parameters: 97 Group 2: Weight Decay: 0.0005 Number of Parameters: 104 Group 3: Bias: 103 Augmentations : Blur: Probability: 0.01 Blur Limit: (3, 7) MedianBlur: Probability: 0.01 Blur Limit: (3, 7) ToGray: Probability: 0.01 CLAHE (Contrast Limited Adaptive Histogram Equalization): Probability: 0.01 Clip Limit: (1, 4.0) Tile Grid Size: (8, 8) Model Architecture Overview This table details the layers and parameters of the YOLOv8 model architecture after overriding the number of classes from 80 to 3. Each row represents a layer in the model, indicating the type of layer, its parameters, and specific arguments. The model is built using various modules such as convolutional layers ( Conv ), concatenation layers ( Concat ), and the Detect module at the end, which specifies the number of classes and their respective parameters. The following table summarizes the architecture of the YOLOv8 model, with the number of classes ( nc ) overridden from 80 to 3. The table details the layers, parameters, and modules used in the model. Layer From Num Params Module Arguments 0 -1 1 1,856 ultralytics.nn.modules.Conv [3, 64, 3, 2] 1 -1 1 73,984 ultralytics.nn.modules.Conv [64, 128, 3, 2] 2 -1 3 279,808 ultralytics.nn.modules.C2f [128, 128, 3, True] 3 -1 1 295,424 ultralytics.nn.modules.Conv [128, 256, 3, 2] 4 -1 6 2,101,248 ultralytics.nn.modules.C2f [256, 256, 6, True] 5 -1 1 1,180,672 ultralytics.nn.modules.Conv [256, 512, 3, 2] 6 -1 6 8,396,800 ultralytics.nn.modules.C2f [512, 512, 6, True] 7 -1 1 2,360,320 ultralytics.nn.modules.Conv [512, 512, 3, 2] 8 -1 3 4,461,568 ultralytics.nn.modules.C2f [512, 512, 3, True] 9 -1 1 656,896 ultralytics.nn.modules.SPPF [512, 512, 5] 10 -1 1 0 torch.nn.modules.upsampling.Upsample [None, 2, 'nearest'] 11 [-1, 6] 1 0 ultralytics.nn.modules.Concat [1] 12 -1 3 4,723,712 ultralytics.nn.modules.C2f [1024, 512, 3] 13 -1 1 0 torch.nn.modules.upsampling.Upsample [None, 2, 'nearest'] 14 [-1, 4] 1 0 ultralytics.nn.modules.Concat [1] 15 -1 3 1,247,744 ultralytics.nn.modules.C2f [768, 256, 3] 16 -1 1 590,336 ultralytics.nn.modules.Conv [256, 256, 3, 2] 17 [-1, 12] 1 0 ultralytics.nn.modules.Concat [1] 18 -1 3 4,592,640 ultralytics.nn.modules.C2f [768, 512, 3] 19 -1 1 2,360,320 ultralytics.nn.modules.Conv [512, 512, 3, 2] 20 [-1, 9] 1 0 ultralytics.nn.modules.Concat [1] 21 -1 3 4,723,712 ultralytics.nn.modules.C2f [1024, 512, 3] 22 [15, 18, 21] 1 5,585,113 ultralytics.nn.modules.Detect [3, [256, 512, 512]] Model Summary : - Total Layers : 365 - Total Parameters : 43,632,153 - Total Gradients : 43,632,137 - GFLOPs : 165.4 After Training model 50 epochs completed in 0.590 hours. Optimizer stripped from runs/detect/train/weights/last.pt, 87.6MB Optimizer stripped from runs/detect/train/weights/best.pt, 87.6MB Validating runs/detect/train/weights/best.pt... Ultralytics YOLOv8.0.81 \ud83d\ude80 Python-3.10.12 torch-2.0.1+cu118 CUDA:0 (Tesla T4, 15102MiB) Model summary (fused): 268 layers, 43608921 parameters, 0 gradients, 164.8 GFLOPs Class Images Instances Box(P R mAP50 mAP50-95): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:07<00:00, 1.55s/it] all 153 776 0.783 0.693 0.758 0.612 Bike_Rider 153 265 0.75 0.668 0.732 0.622 Helmet 153 336 0.836 0.776 0.816 0.592 No_Helmet 153 175 0.761 0.636 0.726 0.621 Speed: 1.4ms preprocess, 14.6ms inference, 0.0ms loss, 1.6ms postprocess per image Dataset Configuration (data.yaml) Key Parameters and Their Meaning names : List of class names in the dataset. Bike_Rider : Represents images containing bike riders. Helmet : Represents images containing helmets. No_Helmet : Represents images containing individuals without helmets. nc : Number of classes in the dataset (3). train : Path to the directory containing training images ( /content/dataset/train/images ). val : Path to the directory containing validation images ( /content/dataset/valid/images ). YOLOv8 Training Configuration (args.yaml) Key Parameters and Their Meaning task : The type of task being performed ( detect ). mode : The mode of operation ( train for training the model). model : The path to the pre-trained model weights ( yolov8l.pt ). data : Path to the dataset configuration file ( /content/dataset/data.yaml ). epochs : Number of training epochs (50). patience : Number of epochs to wait for improvement before stopping (50). batch : Batch size (16). imgsz : Image size for training (640). save : Whether to save the training results (true). save_period : Frequency of saving checkpoints (-1 means only save the last). cache : Whether to cache images (false). device : Compute device to use (null for automatic selection). workers : Number of data loader workers (8). project **: Project directory (null for default). name : Experiment name (null for auto-naming). exist_ok : Whether to overwrite existing experiment directory (false). pretrained : Whether to use a pretrained model (false). optimizer : Optimizer type ( SGD ). verbose : Verbose output during training (true). seed : Random seed for reproducibility (0). deterministic : Ensure deterministic behavior (true). single_cls : Treat the dataset as a single class (false). image_weights : Use weighted image sampling (false). rect : Rectangular training (false). cos_lr : Use cosine learning rate scheduler (false). close_mosaic : Close mosaic augmentation (0). resume : Resume training from last checkpoint (false). amp : Use automatic mixed precision (true). overlap_mask : Use overlap masks (true). mask_ratio : Mask ratio (4). dropout : Dropout rate (0.0). val : Whether to validate during training (true). split : Data split for validation ( val ). save_json : Save results to JSON (false). save_hybrid : Save hybrid results (false). conf : Confidence threshold (null). iou : Intersection over Union threshold (0.7). max_det : Maximum detections per image (300). half : Use half precision (false). dnn : Use OpenCV DNN module (false). plots : Generate plots (true). source : Source of the dataset (null). show : Show results (false). save_txt : Save results in TXT format (false). save_conf : Save confidence scores (false). save_crop : Save cropped images (false). show_labels : Show labels on images (true). show_conf : Show confidence scores on images (true). vid_stride : Video frame stride (1). line_thickness : Line thickness for bounding boxes (3). visualize : Visualize feature maps (false). augment : Augment data (false). agnostic_nms : Class-agnostic non-max suppression (false). classes : Filter by class (null). retina_masks : Use high-resolution masks (false). boxes : Use bounding boxes (true). format : Export format ( torchscript ). keras : Export to Keras format (false). optimize : Optimize the model (false). int8 : Quantize model to int8 (false). dynamic : Use dynamic shapes (false). simplify : Simplify the model (false). opset : ONNX opset version (null). workspace : Workspace size for ONNX export (4). nms : Use non-max suppression (false). lr0 : Initial learning rate (0.01). lrf : Final learning rate (0.01). momentum : Momentum for optimizer (0.937). weight_decay : Weight decay (0.0005). warmup_epochs : Warmup epochs (3.0). warmup_momentum : Warmup momentum (0.8). warmup_bias_lr : Warmup bias learning rate (0.1). box : Box loss gain (7.5). cls : Class loss gain (0.5). dfl : DFL loss gain (1.5). pose : Pose loss gain (12.0). kobj : Keypoint objectness gain (1.0). label_smoothing : Label smoothing (0.0). nbs : Nominal batch size (64). hsv_h : HSV-Hue augmentation (0.015). hsv_s : HSV-Saturation augmentation (0.7). hsv_v : HSV-Value augmentation (0.4). degrees : Degree of rotation for augmentation (0.0). translate : Translation for augmentation (0.1). scale : Scaling for augmentation (0.5). shear : Shear for augmentation (0.0). perspective : Perspective for augmentation (0.0). flipud : Vertical flip probability (0.0). fliplr : Horizontal flip probability (0.5). mosaic : Mosaic augmentation (1.0). mixup : Mixup augmentation (0.0). copy_paste : Copy-paste augmentation (0.0). cfg : Configuration file (null). v5loader : Use YOLOv5 data loader (false). tracker : Tracker configuration ( botsort.yaml ). save_dir : Directory to save results ( runs/detect/train ). Also see 1) https://github.com/Viddesh1/Helmet_test_1 2) https://github.com/Viddesh1/Bike-Helmet-Detection 3) https://github.com/Viddesh1/Bike-Helmet-Detectionv2 4) https://github.com/Viddesh1/Bike-Helmet-Detection-Docs 5) https://drive.google.com/drive/folders/1M4FckJJeyPQTTWqgo6xWhW8L4tf0EJ4l?usp=sharing","title":"BHD"},{"location":"bhd/#bike-rider-helmet-detection-using-yolov8","text":"This repository contains a walk through of implementation of bike rider helmet detection using YOLOv8. As we know that bike riders who do not wear helmet may which result in fatal accidents and death in some cases. The goal is to create a ML/DL model that can detect if a person is wearing helment or not. May sure to select GPU in google colab or kaggle notebook for running through the notebook. As running through the code in local machine with no graphical processing unit will take a very significant ammount of time.","title":"Bike Rider Helmet Detection using YOLOv8"},{"location":"bhd/#methodology","text":"1) Data Collection:- Collect images/videos of bike rider, helmet and no helmet for the model to train upon. 2) Data pre-processing:- Pre-processing and autolabel images and videos using foundation model like DINO and SAM (Segment anything model). 3) Train YOLOv8 Model. 4) Evaluate Target Model. 5) Run Inference on images and videos.","title":"Methodology"},{"location":"bhd/#how-to-setup-codebase-locally","text":"python3 -m venv .venv # Create a virtual environment source .venv/bin/activate # Activate a virtual environment git clone https://github.com/Viddesh1/Helmet_test_1.git cd Helmet_test_1/","title":"How to setup codebase locally?"},{"location":"bhd/#repository-file-structure","text":"Helmet_test_1 # This repository root folder \u251c\u2500\u2500 .git # For managing files by git \u251c\u2500\u2500 .gitignore # Files to be not managed by git \u251c\u2500\u2500 Helmet_how_to_auto_train_yolov8_model_with_autodistill.ipynb # For reference \u251c\u2500\u2500 kaggle_Helmet_how_to_auto_train_yolov8_model_with_autodistill.ipynb # YOLOv8 implementation \u251c\u2500\u2500 README.md # This README.md file itself \u2514\u2500\u2500 tree_all.txt # This file structure from tree -a > tree_all.txt command","title":"Repository File Structure"},{"location":"bhd/#dataset-information","text":"For this project only images and videos are needed and no annotations files are needed. As for this project auto labelling is done by using the Ultralytics framework which uses base model naming Grounded SAM (Segment Anything Model) which is a combination of Grounding DINO (Deeper Into Neural Networks) + SAM (Segment Anything model). The target model is YOLOv8. The datset that is used for this project can be find below:- https://www.kaggle.com/datasets/andrewmvd/helmet-detection This dataset contains 764 images of 2 distinct classes for the objective of helmet detection. Bounding box annotations are provided in the PASCAL VOC format. The classes are: With helmet Without helmet Please generate your own kaggle api key for accessing the dataset with in google colab or kaggle notebook Please take a look at the below python code representing labels for individual ontology. from autodistill.detection import CaptionOntology # \"<description of label>\": \"<label_name>\" # \"bike rider\": \"Bike_Rider\", --> label 0 # \"bike rider and passanger with helmet\": \"Helmet\", --> label 1 # \"bike rider and passanger with no helmet\": \"No_Helmet\" --> label 2 ontology=CaptionOntology({ \"bike rider\": \"Bike_Rider\", \"helmet\": \"Helmet\", \"no helmet\": \"No_Helmet\" })","title":"Dataset Information"},{"location":"bhd/#dataset-file-structure","text":"archive # Root file directory of https://www.kaggle.com/datasets/andrewmvd/helmet-detection \u251c\u2500\u2500 annotations # Annotations based on PASCAL VOC format as XML files \u2502 \u251c\u2500\u2500 BikesHelmets0.xml \u2502 \u251c\u2500\u2500 BikesHelmets100.xml \u2502 \u251c\u2500\u2500 BikesHelmets101.xml \u251c\u2500\u2500 images # Public images for classes helmet and without helmet images \u2502 \u251c\u2500\u2500 BikesHelmets0.png \u2502 \u251c\u2500\u2500 BikesHelmets100.png \u2502 \u251c\u2500\u2500 BikesHelmets101.png \u2514\u2500\u2500 tree_all.txt 2 directories, 1529 files","title":"Dataset File Structure"},{"location":"bhd/#this-repository-output-may-change-in-the-near-future-","text":"https://drive.google.com/drive/folders/1M4FckJJeyPQTTWqgo6xWhW8L4tf0EJ4l?usp=sharing","title":"This repository output may change in the near future:-"},{"location":"bhd/#output-file-structure-for-yolov8","text":"YOLOv8_Helmet_V0 # Main directory of YOLOv8 output \u251c\u2500\u2500 dataset \u2502 \u251c\u2500\u2500 annotations # Empty file \u2502 \u251c\u2500\u2500 data.yaml # Information related to data detection labels and train, valid path \u2502 \u251c\u2500\u2500 images # Empty folder \u2502 \u251c\u2500\u2500 train # Train dataset \u2502 \u2502 \u251c\u2500\u2500 images # Images directory path \u2502 \u2502 \u2502 \u251c\u2500\u2500 BikesHelmets0.jpg \u2502 \u2502 \u2502 \u251c\u2500\u2500 BikesHelmets100.jpg \u2502 \u2502 \u2502 \u251c\u2500\u2500 BikesHelmets101.jpg \u2502 \u2502 \u251c\u2500\u2500 labels # Annotations directory path \u2502 \u2502 \u2502 \u251c\u2500\u2500 BikesHelmets0.txt \u2502 \u2502 \u2502 \u251c\u2500\u2500 BikesHelmets100.txt \u2502 \u2502 \u2502 \u251c\u2500\u2500 BikesHelmets101.txt \u2502 \u2502 \u2514\u2500\u2500 labels.cache \u2502 \u2514\u2500\u2500 valid # Validation dataset for testing trained model \u2502 \u251c\u2500\u2500 images # Validation dataset images \u2502 \u2502 \u251c\u2500\u2500 BikesHelmets103.jpg \u2502 \u2502 \u251c\u2500\u2500 BikesHelmets108.jpg \u2502 \u2502 \u251c\u2500\u2500 BikesHelmets119.jpg \u2502 \u251c\u2500\u2500 labels # Validation dataset labels \u2502 \u2502 \u251c\u2500\u2500 BikesHelmets103.txt \u2502 \u2502 \u251c\u2500\u2500 BikesHelmets108.txt \u2502 \u2502 \u251c\u2500\u2500 BikesHelmets119.txt \u2502 \u2514\u2500\u2500 labels.cache \u251c\u2500\u2500 images # All 764 images form the dataset \u2502 \u251c\u2500\u2500 BikesHelmets0.png \u2502 \u251c\u2500\u2500 BikesHelmets100.png \u2502 \u251c\u2500\u2500 BikesHelmets101.png \u251c\u2500\u2500 runs # Predictions \u2502 \u2514\u2500\u2500 detect \u2502 \u251c\u2500\u2500 predict \u2502 \u2502 \u2514\u2500\u2500 he2.mp4 # Inference upon video 1 \u2502 \u251c\u2500\u2500 predict2 \u2502 \u2502 \u2514\u2500\u2500 test_1.mp4 # Inference upon video 2 \u2502 \u251c\u2500\u2500 predict3 \u2502 \u2502 \u2514\u2500\u2500 test_2.mp4 # Inference upon video 3 \u2502 \u2514\u2500\u2500 train \u2502 \u251c\u2500\u2500 args.yaml # Configuration blue print for training the YOLOv8 model parameters \u2502 \u251c\u2500\u2500 confusion_matrix.png \u2502 \u251c\u2500\u2500 events.out.tfevents.1697046331.428f98cba7b3.163.0 \u2502 \u251c\u2500\u2500 F1_curve.png \u2502 \u251c\u2500\u2500 labels_correlogram.jpg \u2502 \u251c\u2500\u2500 labels.jpg \u2502 \u251c\u2500\u2500 P_curve.png \u2502 \u251c\u2500\u2500 PR_curve.png \u2502 \u251c\u2500\u2500 R_curve.png \u2502 \u251c\u2500\u2500 results.csv # All the metrics for train_loss, class_loss, lr etc \u2502 \u251c\u2500\u2500 results.png # Generated from results.csv \u2502 \u251c\u2500\u2500 train_batch0.jpg \u2502 \u251c\u2500\u2500 train_batch1.jpg \u2502 \u251c\u2500\u2500 train_batch2.jpg \u2502 \u251c\u2500\u2500 val_batch0_labels.jpg \u2502 \u251c\u2500\u2500 val_batch0_pred.jpg \u2502 \u251c\u2500\u2500 val_batch1_labels.jpg \u2502 \u251c\u2500\u2500 val_batch1_pred.jpg \u2502 \u251c\u2500\u2500 val_batch2_labels.jpg \u2502 \u251c\u2500\u2500 val_batch2_pred.jpg \u2502 \u2514\u2500\u2500 weights # Model weights after training on custom dataset \u2502 \u251c\u2500\u2500 best.pt # Best model as pytorch format \u2502 \u2514\u2500\u2500 last.pt # Last model as pytorch format \u251c\u2500\u2500 tree_all.txt # Generated by tree -a > tree_all.txt \u251c\u2500\u2500 videos # Videos to be uploaded for preprocessing \u2502 \u251c\u2500\u2500 he2.mp4 \u2502 \u251c\u2500\u2500 test_1.mp4 \u2502 \u2514\u2500\u2500 test_2.mp4 \u251c\u2500\u2500 yolov8l.pt # Default YOLOV8 large model \u2514\u2500\u2500 yolov8n.pt # Default YOLOv8 nano model","title":"Output File Structure for YOLOv8"},{"location":"bhd/#external-packages-needed-for-this-project","text":"!pip install -q kaggle # For accessing dataset locally in google colab !pip install -U ultralytics !pip install -q \\ # install require packages in quite mode autodistill \\ # Automates model distillation autodistill-grounded-sam \\ # Enhanced distillation with grounding and self-attention mechanisms. autodistill-yolov8 \\ # distilling YOLOv8 models supervision==0.9.0 # For supervising models","title":"External packages needed for this project"},{"location":"bhd/#train-target-model-yolov8","text":"%cd {HOME} from autodistill_yolov8 import YOLOv8 target_model = YOLOv8(\"yolov8l.pt\") target_model.train(DATA_YAML_PATH, epochs=50) #100","title":"Train target model - YOLOv8"},{"location":"bhd/#relevant-information-of-target-model-yolov8","text":"Model Download : File : yolov8l.pt Source : Ultralytics GitHub repository :- (https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8l.pt) Description : Downloads the pre-trained YOLOv8 large model, which is approximately 83.7 MB in size. The model is used for object detection tasks. YOLO library : Current Version : 8.0.81 New Version Available : 8.0.196 Update Command : pip install -U ultralytics Description : Suggests updating the Ultralytics library to the latest version for improved features and performance. Environment Details : Python Version : 3.10.12 PyTorch Version : 2.0.1+cu118 CUDA Version : 0 (CUDA enabled device: Tesla T4 with 15102 MiB memory) Description : Specifies the software and hardware environment used for running the YOLOv8 model. Training Configuration : Task : detect Mode : train Model : yolov8l.pt Data : /content/dataset/data.yaml Epochs : 50 Patience : 50 Batch Size : 16 Image Size : 640 Optimizer : SGD Other Parameters : Various other hyperparameters and settings for training the model. Description : Outlines the configuration for training the YOLOv8 model using the specified dataset, model, and hyperparameters. Font Download : File : Arial.ttf Source : Ultralytics website (https://ultralytics.com/assets/Arial.ttf) Location : /root/.config/Ultralytics/Arial.ttf Description : Downloads a font file required for visualizations and plots generated during the training process. Model Configuration Override : Original nc : 80 New nc : 3 Description : Overrides the number of classes ( nc ) in the model configuration to 3, as specified in the dataset configuration ( data.yaml ). Optimizer Type : SGD Learning Rate : 0.01 Parameter Groups : Group 1: Weight Decay: 0.0 Number of Parameters: 97 Group 2: Weight Decay: 0.0005 Number of Parameters: 104 Group 3: Bias: 103 Augmentations : Blur: Probability: 0.01 Blur Limit: (3, 7) MedianBlur: Probability: 0.01 Blur Limit: (3, 7) ToGray: Probability: 0.01 CLAHE (Contrast Limited Adaptive Histogram Equalization): Probability: 0.01 Clip Limit: (1, 4.0) Tile Grid Size: (8, 8)","title":"Relevant Information of target model YOLOv8"},{"location":"bhd/#model-architecture-overview","text":"This table details the layers and parameters of the YOLOv8 model architecture after overriding the number of classes from 80 to 3. Each row represents a layer in the model, indicating the type of layer, its parameters, and specific arguments. The model is built using various modules such as convolutional layers ( Conv ), concatenation layers ( Concat ), and the Detect module at the end, which specifies the number of classes and their respective parameters. The following table summarizes the architecture of the YOLOv8 model, with the number of classes ( nc ) overridden from 80 to 3. The table details the layers, parameters, and modules used in the model. Layer From Num Params Module Arguments 0 -1 1 1,856 ultralytics.nn.modules.Conv [3, 64, 3, 2] 1 -1 1 73,984 ultralytics.nn.modules.Conv [64, 128, 3, 2] 2 -1 3 279,808 ultralytics.nn.modules.C2f [128, 128, 3, True] 3 -1 1 295,424 ultralytics.nn.modules.Conv [128, 256, 3, 2] 4 -1 6 2,101,248 ultralytics.nn.modules.C2f [256, 256, 6, True] 5 -1 1 1,180,672 ultralytics.nn.modules.Conv [256, 512, 3, 2] 6 -1 6 8,396,800 ultralytics.nn.modules.C2f [512, 512, 6, True] 7 -1 1 2,360,320 ultralytics.nn.modules.Conv [512, 512, 3, 2] 8 -1 3 4,461,568 ultralytics.nn.modules.C2f [512, 512, 3, True] 9 -1 1 656,896 ultralytics.nn.modules.SPPF [512, 512, 5] 10 -1 1 0 torch.nn.modules.upsampling.Upsample [None, 2, 'nearest'] 11 [-1, 6] 1 0 ultralytics.nn.modules.Concat [1] 12 -1 3 4,723,712 ultralytics.nn.modules.C2f [1024, 512, 3] 13 -1 1 0 torch.nn.modules.upsampling.Upsample [None, 2, 'nearest'] 14 [-1, 4] 1 0 ultralytics.nn.modules.Concat [1] 15 -1 3 1,247,744 ultralytics.nn.modules.C2f [768, 256, 3] 16 -1 1 590,336 ultralytics.nn.modules.Conv [256, 256, 3, 2] 17 [-1, 12] 1 0 ultralytics.nn.modules.Concat [1] 18 -1 3 4,592,640 ultralytics.nn.modules.C2f [768, 512, 3] 19 -1 1 2,360,320 ultralytics.nn.modules.Conv [512, 512, 3, 2] 20 [-1, 9] 1 0 ultralytics.nn.modules.Concat [1] 21 -1 3 4,723,712 ultralytics.nn.modules.C2f [1024, 512, 3] 22 [15, 18, 21] 1 5,585,113 ultralytics.nn.modules.Detect [3, [256, 512, 512]] Model Summary : - Total Layers : 365 - Total Parameters : 43,632,153 - Total Gradients : 43,632,137 - GFLOPs : 165.4 After Training model 50 epochs completed in 0.590 hours. Optimizer stripped from runs/detect/train/weights/last.pt, 87.6MB Optimizer stripped from runs/detect/train/weights/best.pt, 87.6MB Validating runs/detect/train/weights/best.pt... Ultralytics YOLOv8.0.81 \ud83d\ude80 Python-3.10.12 torch-2.0.1+cu118 CUDA:0 (Tesla T4, 15102MiB) Model summary (fused): 268 layers, 43608921 parameters, 0 gradients, 164.8 GFLOPs Class Images Instances Box(P R mAP50 mAP50-95): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:07<00:00, 1.55s/it] all 153 776 0.783 0.693 0.758 0.612 Bike_Rider 153 265 0.75 0.668 0.732 0.622 Helmet 153 336 0.836 0.776 0.816 0.592 No_Helmet 153 175 0.761 0.636 0.726 0.621 Speed: 1.4ms preprocess, 14.6ms inference, 0.0ms loss, 1.6ms postprocess per image","title":"Model Architecture Overview"},{"location":"bhd/#dataset-configuration-datayaml","text":"","title":"Dataset Configuration (data.yaml)"},{"location":"bhd/#key-parameters-and-their-meaning","text":"names : List of class names in the dataset. Bike_Rider : Represents images containing bike riders. Helmet : Represents images containing helmets. No_Helmet : Represents images containing individuals without helmets. nc : Number of classes in the dataset (3). train : Path to the directory containing training images ( /content/dataset/train/images ). val : Path to the directory containing validation images ( /content/dataset/valid/images ).","title":"Key Parameters and Their Meaning"},{"location":"bhd/#yolov8-training-configuration-argsyaml","text":"","title":"YOLOv8 Training Configuration (args.yaml)"},{"location":"bhd/#key-parameters-and-their-meaning_1","text":"task : The type of task being performed ( detect ). mode : The mode of operation ( train for training the model). model : The path to the pre-trained model weights ( yolov8l.pt ). data : Path to the dataset configuration file ( /content/dataset/data.yaml ). epochs : Number of training epochs (50). patience : Number of epochs to wait for improvement before stopping (50). batch : Batch size (16). imgsz : Image size for training (640). save : Whether to save the training results (true). save_period : Frequency of saving checkpoints (-1 means only save the last). cache : Whether to cache images (false). device : Compute device to use (null for automatic selection). workers : Number of data loader workers (8). project **: Project directory (null for default). name : Experiment name (null for auto-naming). exist_ok : Whether to overwrite existing experiment directory (false). pretrained : Whether to use a pretrained model (false). optimizer : Optimizer type ( SGD ). verbose : Verbose output during training (true). seed : Random seed for reproducibility (0). deterministic : Ensure deterministic behavior (true). single_cls : Treat the dataset as a single class (false). image_weights : Use weighted image sampling (false). rect : Rectangular training (false). cos_lr : Use cosine learning rate scheduler (false). close_mosaic : Close mosaic augmentation (0). resume : Resume training from last checkpoint (false). amp : Use automatic mixed precision (true). overlap_mask : Use overlap masks (true). mask_ratio : Mask ratio (4). dropout : Dropout rate (0.0). val : Whether to validate during training (true). split : Data split for validation ( val ). save_json : Save results to JSON (false). save_hybrid : Save hybrid results (false). conf : Confidence threshold (null). iou : Intersection over Union threshold (0.7). max_det : Maximum detections per image (300). half : Use half precision (false). dnn : Use OpenCV DNN module (false). plots : Generate plots (true). source : Source of the dataset (null). show : Show results (false). save_txt : Save results in TXT format (false). save_conf : Save confidence scores (false). save_crop : Save cropped images (false). show_labels : Show labels on images (true). show_conf : Show confidence scores on images (true). vid_stride : Video frame stride (1). line_thickness : Line thickness for bounding boxes (3). visualize : Visualize feature maps (false). augment : Augment data (false). agnostic_nms : Class-agnostic non-max suppression (false). classes : Filter by class (null). retina_masks : Use high-resolution masks (false). boxes : Use bounding boxes (true). format : Export format ( torchscript ). keras : Export to Keras format (false). optimize : Optimize the model (false). int8 : Quantize model to int8 (false). dynamic : Use dynamic shapes (false). simplify : Simplify the model (false). opset : ONNX opset version (null). workspace : Workspace size for ONNX export (4). nms : Use non-max suppression (false). lr0 : Initial learning rate (0.01). lrf : Final learning rate (0.01). momentum : Momentum for optimizer (0.937). weight_decay : Weight decay (0.0005). warmup_epochs : Warmup epochs (3.0). warmup_momentum : Warmup momentum (0.8). warmup_bias_lr : Warmup bias learning rate (0.1). box : Box loss gain (7.5). cls : Class loss gain (0.5). dfl : DFL loss gain (1.5). pose : Pose loss gain (12.0). kobj : Keypoint objectness gain (1.0). label_smoothing : Label smoothing (0.0). nbs : Nominal batch size (64). hsv_h : HSV-Hue augmentation (0.015). hsv_s : HSV-Saturation augmentation (0.7). hsv_v : HSV-Value augmentation (0.4). degrees : Degree of rotation for augmentation (0.0). translate : Translation for augmentation (0.1). scale : Scaling for augmentation (0.5). shear : Shear for augmentation (0.0). perspective : Perspective for augmentation (0.0). flipud : Vertical flip probability (0.0). fliplr : Horizontal flip probability (0.5). mosaic : Mosaic augmentation (1.0). mixup : Mixup augmentation (0.0). copy_paste : Copy-paste augmentation (0.0). cfg : Configuration file (null). v5loader : Use YOLOv5 data loader (false). tracker : Tracker configuration ( botsort.yaml ). save_dir : Directory to save results ( runs/detect/train ).","title":"Key Parameters and Their Meaning"},{"location":"bhd/#also-see","text":"1) https://github.com/Viddesh1/Helmet_test_1 2) https://github.com/Viddesh1/Bike-Helmet-Detection 3) https://github.com/Viddesh1/Bike-Helmet-Detectionv2 4) https://github.com/Viddesh1/Bike-Helmet-Detection-Docs 5) https://drive.google.com/drive/folders/1M4FckJJeyPQTTWqgo6xWhW8L4tf0EJ4l?usp=sharing","title":"Also see"},{"location":"dj_bhd/","text":"Bike-Helmet-Detection This repository contains the implementation of Bike helmet detection using YOLOv8 using Django. This project contains django web application where end user can fill a form which contains image and video to be submited at the front-end or through API end point for prediction of the bike rider , helmet and no helmet . Validation is done on both front-end and API as a result only if both the fields are submited for prediction then only detection will be done else appropriate error message are displayed. Images and Videos that submited by end users are then ran inference on by YOLO model and displayed on index page which is same page. For sending valid data via API, images and videos are processed and appropriate predicted image and videos with auto generated id is send back to end user. Key Features 1) Github actions workflow for test with tox. 2) Unit testing test cases is done for testing modules. 3) API is ceated using django-rest-framework. 4) API documentation is created using markdown. 5) coverage report is generated. 6) For sorting imports isort is used. 7) For code formating / linting black is used. 8) For static type checking mypy is used (optional). 9) Respective validations are done to the fields. 10) Containerized with docker. 11) kubernetes.yaml file to test with kubernetes in docker and minikube. Techology used 1) Front end :- HTML, CSS, Bootstrap 2) Backend :- Python (Django, DRF) 3) Database :- Sqlite (Default) Major python libraries used for the project # This are the major packages for this project Django==5.0.1 # For Django web application development ultralytics==8.1.9 # For running inference on YOLOv8 model and getting results. pillow==10.2.0 # For images and videos files # Test Coverage coverage==7.4.3 # For generating test coverage report # Django-rest Framework django-filter==23.5 # Simple way to filter down a queryset based on parameters of user djangorestframework==3.14.0 # For API development Markdown==3.5.2 # pip install markdown # For API documentation # Re-Structing the codebase isort==5.13.2 # For sorting imports black==24.3.0 # For python code formatting / linting tox==4.14.2 # To run tests on different OS and python environments mypy==1.9.0 # Static type checker (optional) File Structure Bike-Helmet-Detection \u251c\u2500\u2500 .git # Git file to version control \u251c\u2500\u2500 .github # Github file for workflows \u2502 \u251c\u2500\u2500 ISSUE_TEMPLATE # Github issue template \u2502 \u2502 \u251c\u2500\u2500 bug_report.yml # bug report issue form template \u2502 \u2502 \u251c\u2500\u2500 config.yml # issue config \u2502 \u2502 \u251c\u2500\u2500 feature_request.yml # feature request form template \u2502 \u2502 \u2514\u2500\u2500 pull_request_template.md # custom PR template \u2502 \u2514\u2500\u2500 workflows # Github workflow to run after a PR \u2502 \u2514\u2500\u2500 main.yml \u251c\u2500\u2500 .gitignore # Gitignore file to ignore any unncessary files by git \u251c\u2500\u2500 ml_project # Django project file \u2502 \u251c\u2500\u2500 compose.yaml # Docker compose file to manage docker containers \u2502 \u251c\u2500\u2500 Dockerfile # Docker file to create a image and run using docker compose or docker build \u2502 \u251c\u2500\u2500 kubernetes.yaml # Kubernetes configuration file for kubernetes, minikube \u2502 \u251c\u2500\u2500 README.Docker.md # Docker md file \u2502 \u251c\u2500\u2500 .dockerignore # Docker ignore file to ignore any unnecessary files by docker \u2502 \u251c\u2500\u2500 .coverage # Hidden coverage file \u2502 \u251c\u2500\u2500 .coveragerc # Coverage configuration file to work on local \u2502 \u251c\u2500\u2500 db.sqlite3 # Default database for django \u2502 \u251c\u2500\u2500 deployment_req.txt # Deployment requirements for docker and any other platform \u2502 \u251c\u2500\u2500 htmlcov # Generated by coverage \"coverage html\" command open index.html to see coverage \u2502 \u251c\u2500\u2500 major_packages.txt # Major packages needed for project \u2502 \u251c\u2500\u2500 manage.py # Django's command-line utility for administrative tasks \u2502 \u251c\u2500\u2500 media # Media static files for django for images or videos \u2502 \u2502 \u251c\u2500\u2500 images \u2502 \u2502 \u2502 \u251c\u2500\u2500 bike_rider_JlJpRzO.jpg \u2502 \u2502 \u2502 \u251c\u2500\u2500 bike_rider.jpg \u2502 \u2502 \u2502 \u251c\u2500\u2500 BikesHelmets118.png \u2502 \u2502 \u2502 \u251c\u2500\u2500 BikesHelmets8.png \u2502 \u2502 \u2502 \u2514\u2500\u2500 test_image.jpg \u2502 \u2502 \u251c\u2500\u2500 pred_images \u2502 \u2502 \u2502 \u251c\u2500\u2500 bike_rider_0XTlKgp.jpg \u2502 \u2502 \u2502 \u251c\u2500\u2500 bike_rider.jpg \u2502 \u2502 \u2502 \u251c\u2500\u2500 BikesHelmets118.png \u2502 \u2502 \u2502 \u2514\u2500\u2500test_pred_image.jpg \u2502 \u2502 \u251c\u2500\u2500 pred_videos \u2502 \u2502 \u2502 \u251c\u2500\u2500 he2_1CjSKgu.avi \u2502 \u2502 \u2502 \u251c\u2500\u2500 he2.avi \u2502 \u2502 \u2502 \u251c\u2500\u2500 test_pred_image.mp4 \u2502 \u2502 \u2502 \u2514\u2500\u2500 test_pred_image_yiShPOt.mp4 \u2502 \u2502 \u2514\u2500\u2500 videos \u2502 \u2502 \u251c\u2500\u2500 he2_M76GncE.mp4 \u2502 \u2502 \u251c\u2500\u2500 he2.mp4 \u2502 \u2502 \u251c\u2500\u2500 test_video_0BeuZlt.mp4 \u2502 \u2502 \u251c\u2500\u2500 test_video.mp4 \u2502 \u2502 \u2514\u2500\u2500test_video_MqdGYhL.mp4 \u2502 \u251c\u2500\u2500 ml_app # Dajngo web app for this project \u2502 \u2502 \u251c\u2500\u2500 admin.py # Django admin utility \u2502 \u2502 \u251c\u2500\u2500 apps.py # Django app utility \u2502 \u2502 \u251c\u2500\u2500 assets # Assets for testing ml_app in UI and API \u2502 \u2502 \u2502 \u251c\u2500\u2500 Images \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 bike_rider.jpg \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 BikesHelmets64.png \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 BikesHelmets66.png \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500 BikesHelmets6.png \u2502 \u2502 \u2502 \u2514\u2500\u2500 Videos \u2502 \u2502 \u2502 \u251c\u2500\u2500 he2.mp4 \u2502 \u2502 \u2502 \u251c\u2500\u2500 test_1.mp4 \u2502 \u2502 \u2502 \u2514\u2500\u2500 test_2.mp4 \u2502 \u2502 \u251c\u2500\u2500 credentials.py # My root username and password \u2502 \u2502 \u251c\u2500\u2500 forms.py # Django form utility for rendering UI forms \u2502 \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2502 \u251c\u2500\u2500 .isort.cfg # Isort configuration file \u2502 \u2502 \u251c\u2500\u2500 migrations # Generated by makemigrations and migrate command for DB \u2502 \u2502 \u2502 \u2514\u2500\u2500 0001_initial.py \u2502 \u2502 \u251c\u2500\u2500 models.py # Database models \u2502 \u2502 \u251c\u2500\u2500 pyproject.toml # Configuration for running tox, mypy \u2502 \u2502 \u251c\u2500\u2500 serializers.py # DRF utility for serilization and deserlization \u2502 \u2502 \u251c\u2500\u2500 standalone_api_test.py # Standalone test for api \u2502 \u2502 \u251c\u2500\u2500 static # static files for webapp \u2502 \u2502 \u2502 \u2514\u2500\u2500 ml_app \u2502 \u2502 \u2502 \u2514\u2500\u2500 style.css # Custom css file \u2502 \u2502 \u251c\u2500\u2500 templates \u2502 \u2502 \u2502 \u2514\u2500\u2500 ml_app \u2502 \u2502 \u2502 \u2514\u2500\u2500 index.html # Custom html file \u2502 \u2502 \u251c\u2500\u2500 tests # Testcases packages for ml_app \u2502 \u2502 \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2502 \u2502 \u251c\u2500\u2500 test_assets # Test images and videos for testcases \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 test_images \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 bike_rider.jpg \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 BikesHelmets64.png \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 BikesHelmets66.png \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500 BikesHelmets6.png \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500 test_videos \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 he2.mp4 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 test_1.mp4 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500 test_2.mp4 \u2502 \u2502 \u2502 \u251c\u2500\u2500 test_forms.py # Testcase for forms \u2502 \u2502 \u2502 \u251c\u2500\u2500 test_model.py # Testcase for models.py file \u2502 \u2502 \u2502 \u251c\u2500\u2500 test_urls.py # Testcase for urls.py \u2502 \u2502 \u2502 \u2514\u2500\u2500 test_views.py # Testcase for views.py \u2502 \u2502 \u251c\u2500\u2500 .tox \u2502 \u2502 \u251c\u2500\u2500 urls.py # Django utility for URL routing \u2502 \u2502 \u251c\u2500\u2500 utils.py # Custom function for YOLOv8 model inference \u2502 \u2502 \u251c\u2500\u2500 views.py # Django utility backend logic for web_app and api \u2502 \u2502 \u251c\u2500\u2500 web_cam.py # Python script to run on local webcam \u2502 \u2502 \u2514\u2500\u2500 weights # Model weights of YOLOv8 \u2502 \u2502 \u251c\u2500\u2500 best.pt # Best model for YOLOv8 \u2502 \u2502 \u251c\u2500\u2500 information.txt \u2502 \u2502 \u2514\u2500\u2500 last.pt # Last model for YOLOv8 \u2502 \u251c\u2500\u2500 ml_project \u2502 \u2502 \u251c\u2500\u2500 asgi.py \u2502 \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2502 \u251c\u2500\u2500 settings.py # Django settings for ml_app \u2502 \u2502 \u251c\u2500\u2500 urls.py # URL routing for ml_app and api \u2502 \u2502 \u2514\u2500\u2500 wsgi.py \u2502 \u251c\u2500\u2500 requirements.txt # pip install -r requirements.txt command for local development \u2502 \u251c\u2500\u2500 runs # Generated by ultralytics yolov8 predictions \u2502 \u2514\u2500\u2500 tox.ini # tox configurations \u251c\u2500\u2500 tree_all.txt # All the file tree structure including hidden files using tree -a > tree_all.txt \u251c\u2500\u2500 tree.txt # File tree structure excluding hidden files using command tree > tree.txt \u2514\u2500\u2500 README.md # This file itself Root username and password USERNAME = \"viddesh\" PASSWORD = \"!@#$%^&*()_+\" Other Dependencies python version :- 3.10.12 (local) Docker version :- Docker version 25.0.3, build 4debf41 minikube version :- v1.32.0 kubectl Client version :- v1.29.2 API end endpoints information 1) http://127.0.0.1:8000/ --> Run's Django webapp 2) http://127.0.0.1:8000/admin/ --> Django admin panel 3) http://127.0.0.1:8000/api/ --> Default router root view 4) http://127.0.0.1:8000/api/images/ --> End user submitted image and video list api 5) http://127.0.0.1:8000/api/predimages/ --> Predicted image and videos api How to run this django project locally? python3 -m venv .venv source .venv/bin/activate unset VIRTUAL_ENV && exec $SHELL # # To deactivate the virtual environment git clone https://github.com/Viddesh1/Bike-Helmet-Detection.git cd Bike-Helmet-Detection/ pip install -r requirements.txt cd ml_project/ python3 manage.py makemigrations python3 manage.py migrate python3 manage.py runserver # web app will run on http://127.0.0.1:8000/ Run Test Cases cd ml_project/ python3 manage.py test ml_app Run isort cd ml_project/ml_app/ isort . Run Black cd ml_project/ml_app/ black . Run tox Github actions takes care of running tox (remote) cd ml_project/ tox OR cd ml_project/ml_app/ tox Run coverage report cd ml_project/ coverage run manage.py test ml_app coverage report -m coverage html # To generate html report Run on Docker Make sure docker demon is running using docker-desktop cd ml_project/ docker compose up Run kubectl using docker in kubernetes Make sure docker demon is running using docker-desktop and kubernetes is enable in docker desktop settings cd ml_project/ kubectl config get-contexts kubectl config use-context docker-desktop kubectl apply -f kubernetes.yaml kubectl get pods Run kubectl using minikube cd ml_project/ kubectl config get-contexts kubectl config use-context minikube minikube start minikube kubectl -- apply --filename my-deployment.yaml minikube stop Inference on Video Inference on Image Demo Index Page Django admin panel Sqlite3 database API end points API Testing from standalone_api_test.py response = requests.get(url = f'{base_url}/api/', auth=(credentials.USERNAME, credentials.PASSWORD)) json_str = json.dumps(response.json(), indent=4) print(json_str) (.venv) viddesh@viddesh-desktop:~/Desktop/Webapp1/Bike-Helmet-Detection/ml_project/ml_app$ python3 standalone_api_test.py { \"images\": \"http://127.0.0.1:8000/api/images/\", \"predimages\": \"http://127.0.0.1:8000/api/predimages/\" } # Sample data for creating an Image image_data = { 'image': open(os.path.join(os.getcwd(), \"tests\", \"test_assets\", \"test_images\", \"bike_rider.jpg\"), 'rb'), 'video': open(os.path.join(os.getcwd(), \"tests\", \"test_assets\", \"test_videos\", 'he2.mp4'), 'rb') } # Send a POST request to create a new Image response = requests.post(image_url, auth=(credentials.USERNAME, credentials.PASSWORD), files=image_data) # print('Image API Response:', response.status_code, response.json()) json_str = json.dumps(response.json(), indent=4) print(json_str) (.venv) viddesh@viddesh-desktop:~/Desktop/Webapp1/Bike-Helmet-Detection/ml_project/ml_app$ python3 standalone_api_test.py { \"id\": 65, \"image\": \"/media/images/bike_rider_pFZJIdi.jpg\", \"video\": \"/media/videos/he2_WEAqdaF.mp4\" } # Get the latest response from GET request. response = requests.get(url = f'{base_url}/api/images/65/', auth=(credentials.USERNAME, credentials.PASSWORD)) json_str = json.dumps(response.json(), indent=4) print(json_str) # GET the POST response that I just did. response = requests.get(url = f'{base_url}/api/predimages/65/', auth=(credentials.USERNAME, credentials.PASSWORD)) print(f\"Status code: {response.status_code}\") json_str = json.dumps(response.json(), indent=4) print(json_str) (.venv) viddesh@viddesh-desktop:~/Desktop/Webapp1/Bike-Helmet-Detection/ml_project/ml_app$ python3 standalone_api_test.py { \"id\": 65, \"image\": \"/media/images/bike_rider_pFZJIdi.jpg\", \"video\": \"/media/videos/he2_WEAqdaF.mp4\" } Status code: 200 { \"pred_image_id\": 65, \"pred_image\": \"http://127.0.0.1:8000/media/pred_images/bike_rider_pFZJIdi.jpg\", \"pred_video\": \"http://127.0.0.1:8000/media/pred_videos/he2_WEAqdaF.avi\" } Also see 1) https://github.com/Viddesh1/Helmet_test_1 2) https://github.com/Viddesh1/Bike-Helmet-Detectionv2 3) https://github.com/Viddesh1/Bike-Helmet-Detection-Docs","title":"Django"},{"location":"dj_bhd/#bike-helmet-detection","text":"This repository contains the implementation of Bike helmet detection using YOLOv8 using Django. This project contains django web application where end user can fill a form which contains image and video to be submited at the front-end or through API end point for prediction of the bike rider , helmet and no helmet . Validation is done on both front-end and API as a result only if both the fields are submited for prediction then only detection will be done else appropriate error message are displayed. Images and Videos that submited by end users are then ran inference on by YOLO model and displayed on index page which is same page. For sending valid data via API, images and videos are processed and appropriate predicted image and videos with auto generated id is send back to end user.","title":"Bike-Helmet-Detection"},{"location":"dj_bhd/#key-features","text":"1) Github actions workflow for test with tox. 2) Unit testing test cases is done for testing modules. 3) API is ceated using django-rest-framework. 4) API documentation is created using markdown. 5) coverage report is generated. 6) For sorting imports isort is used. 7) For code formating / linting black is used. 8) For static type checking mypy is used (optional). 9) Respective validations are done to the fields. 10) Containerized with docker. 11) kubernetes.yaml file to test with kubernetes in docker and minikube.","title":"Key Features"},{"location":"dj_bhd/#techology-used","text":"1) Front end :- HTML, CSS, Bootstrap 2) Backend :- Python (Django, DRF) 3) Database :- Sqlite (Default)","title":"Techology used"},{"location":"dj_bhd/#major-python-libraries-used-for-the-project","text":"# This are the major packages for this project Django==5.0.1 # For Django web application development ultralytics==8.1.9 # For running inference on YOLOv8 model and getting results. pillow==10.2.0 # For images and videos files # Test Coverage coverage==7.4.3 # For generating test coverage report # Django-rest Framework django-filter==23.5 # Simple way to filter down a queryset based on parameters of user djangorestframework==3.14.0 # For API development Markdown==3.5.2 # pip install markdown # For API documentation # Re-Structing the codebase isort==5.13.2 # For sorting imports black==24.3.0 # For python code formatting / linting tox==4.14.2 # To run tests on different OS and python environments mypy==1.9.0 # Static type checker (optional)","title":"Major python libraries used for the project"},{"location":"dj_bhd/#file-structure","text":"Bike-Helmet-Detection \u251c\u2500\u2500 .git # Git file to version control \u251c\u2500\u2500 .github # Github file for workflows \u2502 \u251c\u2500\u2500 ISSUE_TEMPLATE # Github issue template \u2502 \u2502 \u251c\u2500\u2500 bug_report.yml # bug report issue form template \u2502 \u2502 \u251c\u2500\u2500 config.yml # issue config \u2502 \u2502 \u251c\u2500\u2500 feature_request.yml # feature request form template \u2502 \u2502 \u2514\u2500\u2500 pull_request_template.md # custom PR template \u2502 \u2514\u2500\u2500 workflows # Github workflow to run after a PR \u2502 \u2514\u2500\u2500 main.yml \u251c\u2500\u2500 .gitignore # Gitignore file to ignore any unncessary files by git \u251c\u2500\u2500 ml_project # Django project file \u2502 \u251c\u2500\u2500 compose.yaml # Docker compose file to manage docker containers \u2502 \u251c\u2500\u2500 Dockerfile # Docker file to create a image and run using docker compose or docker build \u2502 \u251c\u2500\u2500 kubernetes.yaml # Kubernetes configuration file for kubernetes, minikube \u2502 \u251c\u2500\u2500 README.Docker.md # Docker md file \u2502 \u251c\u2500\u2500 .dockerignore # Docker ignore file to ignore any unnecessary files by docker \u2502 \u251c\u2500\u2500 .coverage # Hidden coverage file \u2502 \u251c\u2500\u2500 .coveragerc # Coverage configuration file to work on local \u2502 \u251c\u2500\u2500 db.sqlite3 # Default database for django \u2502 \u251c\u2500\u2500 deployment_req.txt # Deployment requirements for docker and any other platform \u2502 \u251c\u2500\u2500 htmlcov # Generated by coverage \"coverage html\" command open index.html to see coverage \u2502 \u251c\u2500\u2500 major_packages.txt # Major packages needed for project \u2502 \u251c\u2500\u2500 manage.py # Django's command-line utility for administrative tasks \u2502 \u251c\u2500\u2500 media # Media static files for django for images or videos \u2502 \u2502 \u251c\u2500\u2500 images \u2502 \u2502 \u2502 \u251c\u2500\u2500 bike_rider_JlJpRzO.jpg \u2502 \u2502 \u2502 \u251c\u2500\u2500 bike_rider.jpg \u2502 \u2502 \u2502 \u251c\u2500\u2500 BikesHelmets118.png \u2502 \u2502 \u2502 \u251c\u2500\u2500 BikesHelmets8.png \u2502 \u2502 \u2502 \u2514\u2500\u2500 test_image.jpg \u2502 \u2502 \u251c\u2500\u2500 pred_images \u2502 \u2502 \u2502 \u251c\u2500\u2500 bike_rider_0XTlKgp.jpg \u2502 \u2502 \u2502 \u251c\u2500\u2500 bike_rider.jpg \u2502 \u2502 \u2502 \u251c\u2500\u2500 BikesHelmets118.png \u2502 \u2502 \u2502 \u2514\u2500\u2500test_pred_image.jpg \u2502 \u2502 \u251c\u2500\u2500 pred_videos \u2502 \u2502 \u2502 \u251c\u2500\u2500 he2_1CjSKgu.avi \u2502 \u2502 \u2502 \u251c\u2500\u2500 he2.avi \u2502 \u2502 \u2502 \u251c\u2500\u2500 test_pred_image.mp4 \u2502 \u2502 \u2502 \u2514\u2500\u2500 test_pred_image_yiShPOt.mp4 \u2502 \u2502 \u2514\u2500\u2500 videos \u2502 \u2502 \u251c\u2500\u2500 he2_M76GncE.mp4 \u2502 \u2502 \u251c\u2500\u2500 he2.mp4 \u2502 \u2502 \u251c\u2500\u2500 test_video_0BeuZlt.mp4 \u2502 \u2502 \u251c\u2500\u2500 test_video.mp4 \u2502 \u2502 \u2514\u2500\u2500test_video_MqdGYhL.mp4 \u2502 \u251c\u2500\u2500 ml_app # Dajngo web app for this project \u2502 \u2502 \u251c\u2500\u2500 admin.py # Django admin utility \u2502 \u2502 \u251c\u2500\u2500 apps.py # Django app utility \u2502 \u2502 \u251c\u2500\u2500 assets # Assets for testing ml_app in UI and API \u2502 \u2502 \u2502 \u251c\u2500\u2500 Images \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 bike_rider.jpg \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 BikesHelmets64.png \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 BikesHelmets66.png \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500 BikesHelmets6.png \u2502 \u2502 \u2502 \u2514\u2500\u2500 Videos \u2502 \u2502 \u2502 \u251c\u2500\u2500 he2.mp4 \u2502 \u2502 \u2502 \u251c\u2500\u2500 test_1.mp4 \u2502 \u2502 \u2502 \u2514\u2500\u2500 test_2.mp4 \u2502 \u2502 \u251c\u2500\u2500 credentials.py # My root username and password \u2502 \u2502 \u251c\u2500\u2500 forms.py # Django form utility for rendering UI forms \u2502 \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2502 \u251c\u2500\u2500 .isort.cfg # Isort configuration file \u2502 \u2502 \u251c\u2500\u2500 migrations # Generated by makemigrations and migrate command for DB \u2502 \u2502 \u2502 \u2514\u2500\u2500 0001_initial.py \u2502 \u2502 \u251c\u2500\u2500 models.py # Database models \u2502 \u2502 \u251c\u2500\u2500 pyproject.toml # Configuration for running tox, mypy \u2502 \u2502 \u251c\u2500\u2500 serializers.py # DRF utility for serilization and deserlization \u2502 \u2502 \u251c\u2500\u2500 standalone_api_test.py # Standalone test for api \u2502 \u2502 \u251c\u2500\u2500 static # static files for webapp \u2502 \u2502 \u2502 \u2514\u2500\u2500 ml_app \u2502 \u2502 \u2502 \u2514\u2500\u2500 style.css # Custom css file \u2502 \u2502 \u251c\u2500\u2500 templates \u2502 \u2502 \u2502 \u2514\u2500\u2500 ml_app \u2502 \u2502 \u2502 \u2514\u2500\u2500 index.html # Custom html file \u2502 \u2502 \u251c\u2500\u2500 tests # Testcases packages for ml_app \u2502 \u2502 \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2502 \u2502 \u251c\u2500\u2500 test_assets # Test images and videos for testcases \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 test_images \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 bike_rider.jpg \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 BikesHelmets64.png \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 BikesHelmets66.png \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500 BikesHelmets6.png \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500 test_videos \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 he2.mp4 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 test_1.mp4 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500 test_2.mp4 \u2502 \u2502 \u2502 \u251c\u2500\u2500 test_forms.py # Testcase for forms \u2502 \u2502 \u2502 \u251c\u2500\u2500 test_model.py # Testcase for models.py file \u2502 \u2502 \u2502 \u251c\u2500\u2500 test_urls.py # Testcase for urls.py \u2502 \u2502 \u2502 \u2514\u2500\u2500 test_views.py # Testcase for views.py \u2502 \u2502 \u251c\u2500\u2500 .tox \u2502 \u2502 \u251c\u2500\u2500 urls.py # Django utility for URL routing \u2502 \u2502 \u251c\u2500\u2500 utils.py # Custom function for YOLOv8 model inference \u2502 \u2502 \u251c\u2500\u2500 views.py # Django utility backend logic for web_app and api \u2502 \u2502 \u251c\u2500\u2500 web_cam.py # Python script to run on local webcam \u2502 \u2502 \u2514\u2500\u2500 weights # Model weights of YOLOv8 \u2502 \u2502 \u251c\u2500\u2500 best.pt # Best model for YOLOv8 \u2502 \u2502 \u251c\u2500\u2500 information.txt \u2502 \u2502 \u2514\u2500\u2500 last.pt # Last model for YOLOv8 \u2502 \u251c\u2500\u2500 ml_project \u2502 \u2502 \u251c\u2500\u2500 asgi.py \u2502 \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2502 \u251c\u2500\u2500 settings.py # Django settings for ml_app \u2502 \u2502 \u251c\u2500\u2500 urls.py # URL routing for ml_app and api \u2502 \u2502 \u2514\u2500\u2500 wsgi.py \u2502 \u251c\u2500\u2500 requirements.txt # pip install -r requirements.txt command for local development \u2502 \u251c\u2500\u2500 runs # Generated by ultralytics yolov8 predictions \u2502 \u2514\u2500\u2500 tox.ini # tox configurations \u251c\u2500\u2500 tree_all.txt # All the file tree structure including hidden files using tree -a > tree_all.txt \u251c\u2500\u2500 tree.txt # File tree structure excluding hidden files using command tree > tree.txt \u2514\u2500\u2500 README.md # This file itself","title":"File Structure"},{"location":"dj_bhd/#root-username-and-password","text":"USERNAME = \"viddesh\" PASSWORD = \"!@#$%^&*()_+\"","title":"Root username and password"},{"location":"dj_bhd/#other-dependencies","text":"python version :- 3.10.12 (local) Docker version :- Docker version 25.0.3, build 4debf41 minikube version :- v1.32.0 kubectl Client version :- v1.29.2","title":"Other Dependencies"},{"location":"dj_bhd/#api-end-endpoints-information","text":"1) http://127.0.0.1:8000/ --> Run's Django webapp 2) http://127.0.0.1:8000/admin/ --> Django admin panel 3) http://127.0.0.1:8000/api/ --> Default router root view 4) http://127.0.0.1:8000/api/images/ --> End user submitted image and video list api 5) http://127.0.0.1:8000/api/predimages/ --> Predicted image and videos api","title":"API end endpoints information"},{"location":"dj_bhd/#how-to-run-this-django-project-locally","text":"python3 -m venv .venv source .venv/bin/activate unset VIRTUAL_ENV && exec $SHELL # # To deactivate the virtual environment git clone https://github.com/Viddesh1/Bike-Helmet-Detection.git cd Bike-Helmet-Detection/ pip install -r requirements.txt cd ml_project/ python3 manage.py makemigrations python3 manage.py migrate python3 manage.py runserver # web app will run on http://127.0.0.1:8000/","title":"How to run this django project locally?"},{"location":"dj_bhd/#run-test-cases","text":"cd ml_project/ python3 manage.py test ml_app","title":"Run Test Cases"},{"location":"dj_bhd/#run-isort","text":"cd ml_project/ml_app/ isort .","title":"Run isort"},{"location":"dj_bhd/#run-black","text":"cd ml_project/ml_app/ black .","title":"Run Black"},{"location":"dj_bhd/#run-tox","text":"","title":"Run tox"},{"location":"dj_bhd/#github-actions-takes-care-of-running-tox-remote","text":"cd ml_project/ tox OR cd ml_project/ml_app/ tox","title":"Github actions takes care of running tox (remote)"},{"location":"dj_bhd/#run-coverage-report","text":"cd ml_project/ coverage run manage.py test ml_app coverage report -m coverage html # To generate html report","title":"Run coverage report"},{"location":"dj_bhd/#run-on-docker","text":"","title":"Run on Docker"},{"location":"dj_bhd/#make-sure-docker-demon-is-running-using-docker-desktop","text":"cd ml_project/ docker compose up","title":"Make sure docker demon is running using docker-desktop"},{"location":"dj_bhd/#run-kubectl-using-docker-in-kubernetes","text":"","title":"Run kubectl using docker in kubernetes"},{"location":"dj_bhd/#make-sure-docker-demon-is-running-using-docker-desktop-and-kubernetes-is-enable-in-docker-desktop-settings","text":"cd ml_project/ kubectl config get-contexts kubectl config use-context docker-desktop kubectl apply -f kubernetes.yaml kubectl get pods","title":"Make sure docker demon is running using docker-desktop and kubernetes is enable in docker desktop settings"},{"location":"dj_bhd/#run-kubectl-using-minikube","text":"cd ml_project/ kubectl config get-contexts kubectl config use-context minikube minikube start minikube kubectl -- apply --filename my-deployment.yaml minikube stop","title":"Run kubectl using minikube"},{"location":"dj_bhd/#inference-on-video","text":"","title":"Inference on Video"},{"location":"dj_bhd/#inference-on-image","text":"","title":"Inference on Image"},{"location":"dj_bhd/#demo","text":"","title":"Demo"},{"location":"dj_bhd/#index-page","text":"","title":"Index Page"},{"location":"dj_bhd/#django-admin-panel","text":"","title":"Django admin panel"},{"location":"dj_bhd/#sqlite3-database","text":"","title":"Sqlite3 database"},{"location":"dj_bhd/#api-end-points","text":"","title":"API end points"},{"location":"dj_bhd/#api-testing-from-standalone_api_testpy","text":"response = requests.get(url = f'{base_url}/api/', auth=(credentials.USERNAME, credentials.PASSWORD)) json_str = json.dumps(response.json(), indent=4) print(json_str) (.venv) viddesh@viddesh-desktop:~/Desktop/Webapp1/Bike-Helmet-Detection/ml_project/ml_app$ python3 standalone_api_test.py { \"images\": \"http://127.0.0.1:8000/api/images/\", \"predimages\": \"http://127.0.0.1:8000/api/predimages/\" } # Sample data for creating an Image image_data = { 'image': open(os.path.join(os.getcwd(), \"tests\", \"test_assets\", \"test_images\", \"bike_rider.jpg\"), 'rb'), 'video': open(os.path.join(os.getcwd(), \"tests\", \"test_assets\", \"test_videos\", 'he2.mp4'), 'rb') } # Send a POST request to create a new Image response = requests.post(image_url, auth=(credentials.USERNAME, credentials.PASSWORD), files=image_data) # print('Image API Response:', response.status_code, response.json()) json_str = json.dumps(response.json(), indent=4) print(json_str) (.venv) viddesh@viddesh-desktop:~/Desktop/Webapp1/Bike-Helmet-Detection/ml_project/ml_app$ python3 standalone_api_test.py { \"id\": 65, \"image\": \"/media/images/bike_rider_pFZJIdi.jpg\", \"video\": \"/media/videos/he2_WEAqdaF.mp4\" } # Get the latest response from GET request. response = requests.get(url = f'{base_url}/api/images/65/', auth=(credentials.USERNAME, credentials.PASSWORD)) json_str = json.dumps(response.json(), indent=4) print(json_str) # GET the POST response that I just did. response = requests.get(url = f'{base_url}/api/predimages/65/', auth=(credentials.USERNAME, credentials.PASSWORD)) print(f\"Status code: {response.status_code}\") json_str = json.dumps(response.json(), indent=4) print(json_str) (.venv) viddesh@viddesh-desktop:~/Desktop/Webapp1/Bike-Helmet-Detection/ml_project/ml_app$ python3 standalone_api_test.py { \"id\": 65, \"image\": \"/media/images/bike_rider_pFZJIdi.jpg\", \"video\": \"/media/videos/he2_WEAqdaF.mp4\" } Status code: 200 { \"pred_image_id\": 65, \"pred_image\": \"http://127.0.0.1:8000/media/pred_images/bike_rider_pFZJIdi.jpg\", \"pred_video\": \"http://127.0.0.1:8000/media/pred_videos/he2_WEAqdaF.avi\" }","title":"API Testing from standalone_api_test.py"},{"location":"dj_bhd/#also-see","text":"1) https://github.com/Viddesh1/Helmet_test_1 2) https://github.com/Viddesh1/Bike-Helmet-Detectionv2 3) https://github.com/Viddesh1/Bike-Helmet-Detection-Docs","title":"Also see"},{"location":"index_2/","text":"Bike Rider Helmet Detection Computer Vision: Introduction Computer Vision is an interdisciplinary field of study that enables computers to interpret and understand visual information from the world, much like the human visual system. It encompasses the development of algorithms, models, and systems that can process, analyze, and extract meaningful insights from visual data, typically in the form of images and videos. Computer Vision has wide-ranging applications across various industries, including healthcare, automotive, entertainment, surveillance, robotics, and more. This overview provides a comprehensive understanding of Computer Vision, its key components, applications, challenges, and future prospects. Objective / Motivation Problem: Bike riders drivers who do not wear helmet which may result in fatal accidents and death in some cases. Goal: Create a ML/DL model that an detect if a person is wearing helmet or not. Related Work Key Components of Computer Vision: Image Acquisition: Computer Vision begins with the acquisition of visual data, which can come from various sources, including cameras, sensors, or image databases. Image Preprocessing: Raw visual data often requires preprocessing to enhance quality, remove noise, and prepare it for analysis. This includes tasks like image resizing, filtering, and color correction. Feature Extraction: Feature extraction involves identifying and isolating relevant visual patterns or features within an image, such as edges, corners, or texture. Object Detection and Recognition: This component focuses on identifying and classifying objects within images or videos. Object detection and recognition algorithms enable computers to recognize and label objects, faces, or specific patterns. Image Segmentation: Image segmentation divides an image into meaningful regions or segments. It's crucial for tasks like medical image analysis, where different parts of an image may represent different anatomical structures. Motion Analysis: Motion analysis techniques track moving objects and can be used in applications like surveillance, sports analysis, and robotics. Scene Understanding: This involves higher-level interpretation of images or videos to understand the context and relationships between objects within a scene. Methodology 1) Data Collection:- Collect publicly availabel images / videos of bike rider, helmet and no helmet for the model to train upon. 2) Data pre-processing:- Pre-processing and autolabel images and videos using foundation model like DINO and SAM (Segment anything model). 3) Train YOLOv8 Model. 4) Evaluate Target Model. 5) Run Inference on images and videos. Tools and Technologies Hardware Requirements :- 1) Desktop / Laptop / Server 2) 8 GB RAM at least 3) 150 GB Disk space or higher 4) Any processor Intel i5 / AMD 5) Google GPU - Tesla T4 Sofware Requirements :- 1) Windows / Ubuntu os (64 or 32 bit) 2) Google colab / Kaggle jupyter notebook 3) Python 3.10.12 or higher 4) Visual studio code editor, jupyter notebook 5) Sqlite version 0.5.6 or higher Implementation details What is YOLOv8? YOLOv8 is from the YOLO family of models and was released on January 10, 2023. YOLO stands for You Only Look Once, and this series of models are thus named because of their ability to predict every object present in an image with one forward pass. Why YOLOv8? YOLOv8 by ultalytics is a state-of-the-art deep learning model designed for real-time object detection in computer vision applications. With its advanced architecture and cutting-edge algorithms, YOLOv8 has revolutionized the field of object detection, enabling accurate and efficient detection of objects in real-time scenarios. YOLOv8 is quite stable as compare to latest YOLOv9 and recent YOLOv10. Dataset information Dataset :- https://www.kaggle.com/datasets/andrewmvd/helmet-detection It has 764 images of various Bike rider, Rider wearing helmet, Rider not wearing helmet. Also, only images are needed for this project and no annotations are needed from the data as annotation are generated by ultralytics framework called as Autodistill which uses Grounding SAM which is combination of Grounding DiNO and SAM (Segment Anything Model) from meta for autolabel dataset and train on YOLOv8. Autodistill uses big, slower foundation models to train small, faster supervised models. Using autodistill, you can go from unlabeled images to inference on a custom model running at the edge with no human intervention in between. As foundation models get better and better they will increasingly be able to augment or replace humans in the labeling process. This project consist of a combination of both the images and videos images. Video is processed is such a way in which every 3 (can be changed through code) frame are considered as data for the model to train upon. For autolabelling the dataset of images and video frame autodistill uses something which is called as ontology Ontology - an Ontology defines how your Base Model like Grounding SAM is prompted, what your Dataset will describe, and what your Target Model like YOLOv8 will predict. A simple Ontology is the CaptionOntology which prompts a Base Model with text captions and maps them to class names. Other Ontologies may, for instance, use a CLIP vector or example images instead of a text caption. from autodistill.detection import CaptionOntology # \"<description of label>\": \"<label_name>\" # \"bike rider\": \"Bike_Rider\", --> label 0 # \"bike rider and passanger with helmet\": \"Helmet\", --> label 1 # \"bike rider and passanger with no helmet\": \"No_Helmet\" --> label 2 ontology=CaptionOntology({ \"bike rider\": \"Bike_Rider\", \"helmet\": \"Helmet\", \"no helmet\": \"No_Helmet\" }) What are base models? Base Model - A Base Model is a large foundation model that knows a lot about a lot. Base models are often multimodal and can perform many tasks. They're large, slow, and expensive. Examples of Base Models are GroundedSAM and GPT-4's upcoming multimodal variant. We use a Base Model (along with unlabeled input data and an Ontology) to create a Dataset. What are target models? Target Model - a Target Model is a supervised model that consumes a Dataset and outputs a distilled model that is ready for deployment. Target Models are usually small, fast, and fine-tuned to perform a specific task very well (but they don't generalize well beyond the information described in their Dataset). Examples of Target Models are YOLOv8 and DETR. How pre-processing of the dataset is done? Dataset is a set of auto-labeled data that can be used to train a Target Model. It is the output generated by a Base Model. The dataset is then divided into training and validation dataset to check the performance of model. The YOLOv8 model is fine-tuned for custom dataset target values of bike rider , helmet and no helmet . Based on which best.pt and last.pt pytorch training weights are generated. For more details regarding the jupyter notebook implementation. Take a look at the below github url https://github.com/Viddesh1/Helmet_test_1 Fore more details regarding the implementation of the bike helmet detection web application using django. Take a look at the below github url:- https://github.com/Viddesh1/Bike-Helmet-Detection For more details regarding the implementation of the bike helmet detection web application using streamlit. Take a look at the below github url. https://github.com/Viddesh1/Bike-Helmet-Detectionv2.git YOLOv8 Architecture YOLOv8 architecture are too big please use Netron.app for more detail and simply open best.pt and last.pt model: https://netron.app/?ref=blog.roboflow.com best.pt model architecture last.pt model architecture Analysis of the result Inference on images Inference on Video Conclusion In conclusion, the development of a computer vision model using YOLOv8 for bike rider helmet detection represents a significant advancement in enhancing safety measures for riders. The model demonstrates impressive accuracy and efficiency in identifying helmet-wearing individuals, which is crucial for ensuring compliance with safety regulations and reducing the risk of head injuries in bike-related accidents. This project highlights the potential of computer vision technology to address real-world safety challenges effectively. Future enhancement YOLOv9 implementation YOLOv10 implementation Program code 1) Implementation of Bike Helmet Detection Jupyter Notebook https://github.com/Viddesh1/Helmet_test_1 2) Output generated by YOLOv8 https://drive.google.com/drive/folders/1M4FckJJeyPQTTWqgo6xWhW8L4tf0EJ4l?usp=sharing 3) Bike Helmet Detection Django Web application https://github.com/Viddesh1/Bike-Helmet-Detection 4) Bike Helmet Detection Stremlit Web application https://github.com/Viddesh1/Bike-Helmet-Detectionv2 5) Hosted on Streamlit https://bike-helmet-detectionv2-dmehozp3lkef4wnssaepjf.streamlit.app/ 6) Overall Documentation https://github.com/Viddesh1/Bike-Helmet-Detection-Docs References 1) https://github.com/ultralytics/ultralytics 2) https://towardsdatascience.com/dino-emerging-properties-in-self-supervised-vision-transformers-summary-ab91df82cc3c 3) https://github.com/facebookresearch/dino 4) Emerging Properties in Self-Supervised Vision Transformers :- https://arxiv.org/abs/2104.14294 5) https://dinov2.metademolab.com/ 6) https://segment-anything.com/","title":"Index 2"},{"location":"index_2/#bike-rider-helmet-detection","text":"","title":"Bike Rider Helmet Detection"},{"location":"index_2/#computer-vision-introduction","text":"Computer Vision is an interdisciplinary field of study that enables computers to interpret and understand visual information from the world, much like the human visual system. It encompasses the development of algorithms, models, and systems that can process, analyze, and extract meaningful insights from visual data, typically in the form of images and videos. Computer Vision has wide-ranging applications across various industries, including healthcare, automotive, entertainment, surveillance, robotics, and more. This overview provides a comprehensive understanding of Computer Vision, its key components, applications, challenges, and future prospects.","title":"Computer Vision: Introduction"},{"location":"index_2/#objective-motivation","text":"Problem: Bike riders drivers who do not wear helmet which may result in fatal accidents and death in some cases. Goal: Create a ML/DL model that an detect if a person is wearing helmet or not.","title":"Objective / Motivation"},{"location":"index_2/#related-work","text":"Key Components of Computer Vision: Image Acquisition: Computer Vision begins with the acquisition of visual data, which can come from various sources, including cameras, sensors, or image databases. Image Preprocessing: Raw visual data often requires preprocessing to enhance quality, remove noise, and prepare it for analysis. This includes tasks like image resizing, filtering, and color correction. Feature Extraction: Feature extraction involves identifying and isolating relevant visual patterns or features within an image, such as edges, corners, or texture. Object Detection and Recognition: This component focuses on identifying and classifying objects within images or videos. Object detection and recognition algorithms enable computers to recognize and label objects, faces, or specific patterns. Image Segmentation: Image segmentation divides an image into meaningful regions or segments. It's crucial for tasks like medical image analysis, where different parts of an image may represent different anatomical structures. Motion Analysis: Motion analysis techniques track moving objects and can be used in applications like surveillance, sports analysis, and robotics. Scene Understanding: This involves higher-level interpretation of images or videos to understand the context and relationships between objects within a scene.","title":"Related Work"},{"location":"index_2/#methodology","text":"1) Data Collection:- Collect publicly availabel images / videos of bike rider, helmet and no helmet for the model to train upon. 2) Data pre-processing:- Pre-processing and autolabel images and videos using foundation model like DINO and SAM (Segment anything model). 3) Train YOLOv8 Model. 4) Evaluate Target Model. 5) Run Inference on images and videos.","title":"Methodology"},{"location":"index_2/#tools-and-technologies","text":"","title":"Tools and Technologies"},{"location":"index_2/#hardware-requirements-","text":"1) Desktop / Laptop / Server 2) 8 GB RAM at least 3) 150 GB Disk space or higher 4) Any processor Intel i5 / AMD 5) Google GPU - Tesla T4","title":"Hardware Requirements :-"},{"location":"index_2/#sofware-requirements-","text":"1) Windows / Ubuntu os (64 or 32 bit) 2) Google colab / Kaggle jupyter notebook 3) Python 3.10.12 or higher 4) Visual studio code editor, jupyter notebook 5) Sqlite version 0.5.6 or higher","title":"Sofware Requirements :-"},{"location":"index_2/#implementation-details","text":"","title":"Implementation details"},{"location":"index_2/#what-is-yolov8","text":"YOLOv8 is from the YOLO family of models and was released on January 10, 2023. YOLO stands for You Only Look Once, and this series of models are thus named because of their ability to predict every object present in an image with one forward pass.","title":"What is YOLOv8?"},{"location":"index_2/#why-yolov8","text":"YOLOv8 by ultalytics is a state-of-the-art deep learning model designed for real-time object detection in computer vision applications. With its advanced architecture and cutting-edge algorithms, YOLOv8 has revolutionized the field of object detection, enabling accurate and efficient detection of objects in real-time scenarios. YOLOv8 is quite stable as compare to latest YOLOv9 and recent YOLOv10.","title":"Why YOLOv8?"},{"location":"index_2/#dataset-information","text":"Dataset :- https://www.kaggle.com/datasets/andrewmvd/helmet-detection It has 764 images of various Bike rider, Rider wearing helmet, Rider not wearing helmet. Also, only images are needed for this project and no annotations are needed from the data as annotation are generated by ultralytics framework called as Autodistill which uses Grounding SAM which is combination of Grounding DiNO and SAM (Segment Anything Model) from meta for autolabel dataset and train on YOLOv8. Autodistill uses big, slower foundation models to train small, faster supervised models. Using autodistill, you can go from unlabeled images to inference on a custom model running at the edge with no human intervention in between. As foundation models get better and better they will increasingly be able to augment or replace humans in the labeling process. This project consist of a combination of both the images and videos images. Video is processed is such a way in which every 3 (can be changed through code) frame are considered as data for the model to train upon. For autolabelling the dataset of images and video frame autodistill uses something which is called as ontology Ontology - an Ontology defines how your Base Model like Grounding SAM is prompted, what your Dataset will describe, and what your Target Model like YOLOv8 will predict. A simple Ontology is the CaptionOntology which prompts a Base Model with text captions and maps them to class names. Other Ontologies may, for instance, use a CLIP vector or example images instead of a text caption. from autodistill.detection import CaptionOntology # \"<description of label>\": \"<label_name>\" # \"bike rider\": \"Bike_Rider\", --> label 0 # \"bike rider and passanger with helmet\": \"Helmet\", --> label 1 # \"bike rider and passanger with no helmet\": \"No_Helmet\" --> label 2 ontology=CaptionOntology({ \"bike rider\": \"Bike_Rider\", \"helmet\": \"Helmet\", \"no helmet\": \"No_Helmet\" }) What are base models? Base Model - A Base Model is a large foundation model that knows a lot about a lot. Base models are often multimodal and can perform many tasks. They're large, slow, and expensive. Examples of Base Models are GroundedSAM and GPT-4's upcoming multimodal variant. We use a Base Model (along with unlabeled input data and an Ontology) to create a Dataset. What are target models? Target Model - a Target Model is a supervised model that consumes a Dataset and outputs a distilled model that is ready for deployment. Target Models are usually small, fast, and fine-tuned to perform a specific task very well (but they don't generalize well beyond the information described in their Dataset). Examples of Target Models are YOLOv8 and DETR. How pre-processing of the dataset is done? Dataset is a set of auto-labeled data that can be used to train a Target Model. It is the output generated by a Base Model. The dataset is then divided into training and validation dataset to check the performance of model. The YOLOv8 model is fine-tuned for custom dataset target values of bike rider , helmet and no helmet . Based on which best.pt and last.pt pytorch training weights are generated. For more details regarding the jupyter notebook implementation. Take a look at the below github url https://github.com/Viddesh1/Helmet_test_1 Fore more details regarding the implementation of the bike helmet detection web application using django. Take a look at the below github url:- https://github.com/Viddesh1/Bike-Helmet-Detection For more details regarding the implementation of the bike helmet detection web application using streamlit. Take a look at the below github url. https://github.com/Viddesh1/Bike-Helmet-Detectionv2.git","title":"Dataset information"},{"location":"index_2/#yolov8-architecture","text":"YOLOv8 architecture are too big please use Netron.app for more detail and simply open best.pt and last.pt model: https://netron.app/?ref=blog.roboflow.com","title":"YOLOv8 Architecture"},{"location":"index_2/#bestpt-model-architecture","text":"","title":"best.pt model architecture"},{"location":"index_2/#lastpt-model-architecture","text":"","title":"last.pt model architecture"},{"location":"index_2/#analysis-of-the-result","text":"","title":"Analysis of the result"},{"location":"index_2/#inference-on-images","text":"","title":"Inference on images"},{"location":"index_2/#inference-on-video","text":"","title":"Inference on Video"},{"location":"index_2/#conclusion","text":"In conclusion, the development of a computer vision model using YOLOv8 for bike rider helmet detection represents a significant advancement in enhancing safety measures for riders. The model demonstrates impressive accuracy and efficiency in identifying helmet-wearing individuals, which is crucial for ensuring compliance with safety regulations and reducing the risk of head injuries in bike-related accidents. This project highlights the potential of computer vision technology to address real-world safety challenges effectively.","title":"Conclusion"},{"location":"index_2/#future-enhancement","text":"YOLOv9 implementation YOLOv10 implementation","title":"Future enhancement"},{"location":"index_2/#program-code","text":"1) Implementation of Bike Helmet Detection Jupyter Notebook https://github.com/Viddesh1/Helmet_test_1 2) Output generated by YOLOv8 https://drive.google.com/drive/folders/1M4FckJJeyPQTTWqgo6xWhW8L4tf0EJ4l?usp=sharing 3) Bike Helmet Detection Django Web application https://github.com/Viddesh1/Bike-Helmet-Detection 4) Bike Helmet Detection Stremlit Web application https://github.com/Viddesh1/Bike-Helmet-Detectionv2 5) Hosted on Streamlit https://bike-helmet-detectionv2-dmehozp3lkef4wnssaepjf.streamlit.app/ 6) Overall Documentation https://github.com/Viddesh1/Bike-Helmet-Detection-Docs","title":"Program code"},{"location":"index_2/#references","text":"1) https://github.com/ultralytics/ultralytics 2) https://towardsdatascience.com/dino-emerging-properties-in-self-supervised-vision-transformers-summary-ab91df82cc3c 3) https://github.com/facebookresearch/dino 4) Emerging Properties in Self-Supervised Vision Transformers :- https://arxiv.org/abs/2104.14294 5) https://dinov2.metademolab.com/ 6) https://segment-anything.com/","title":"References"},{"location":"st_bhd/","text":"Bike-Helmet-Detectionv2 This repository contains the Bike Helmet detection using YOLOv8 in streamlit. This web application does processing on end users images and videos to detect bike rider , helmet and *no helmet and render processed images and videos to user. Major python libraries used for the project # Below is the list of the major packages needed for working in this project. ultralytics==8.1.8 # For running inference using YOLOv8 model streamlit==1.30.0 # Web application pillow==10.2.0 # For managing images and videos pytube==15.0.0 # For running inference on small youtube videos File Structure Bike-Helmet-Detectionv2 # Root folder of this github repository \u251c\u2500\u2500 app.py # Start page of the application \u251c\u2500\u2500 assets # Necessary assets for testing \u2502 \u251c\u2500\u2500 BikesHelmets6.png \u2502 \u251c\u2500\u2500 video_1.mp4 \u2502 \u251c\u2500\u2500 video_2.mp4 \u2502 \u2514\u2500\u2500 video_3.mp4 \u251c\u2500\u2500 .git # For managing files by git \u251c\u2500\u2500 .gitignore # Files to be not managed by git \u251c\u2500\u2500 helper.py \u251c\u2500\u2500 images # For rendering Predicted and not predicted to streamlit user interface \u2502 \u251c\u2500\u2500 BikesHelmets6_detected.jpg \u2502 \u2514\u2500\u2500 BikesHelmets6.png \u251c\u2500\u2500 local_requirements.txt # For local machine dependencies installation \u251c\u2500\u2500 major_packages.txt # major packages needed for working in this project \u251c\u2500\u2500 packages.txt # Externally insatallion of package by streamlit for deployment \u251c\u2500\u2500 README.md # This readme.md file itself \u251c\u2500\u2500 requirements.txt # For deployment dependencies file \u251c\u2500\u2500 runs \u2502 \u2514\u2500\u2500 detect \u2502 \u2514\u2500\u2500 predict \u2502 \u2514\u2500\u2500 BikesHelmets6.png # Predicted image \u251c\u2500\u2500 settings.py # Configuration blue print for this streamlit web application \u251c\u2500\u2500 videos # For rendering video into streamlit user interface \u2502 \u251c\u2500\u2500 video_1.mp4 \u2502 \u251c\u2500\u2500 video_2.mp4 \u2502 \u2514\u2500\u2500 video_3.mp4 \u2514\u2500\u2500 weights # Model weights \u251c\u2500\u2500 best.pt # Best model weight as pytorch format \u251c\u2500\u2500 information.txt # Information related to placing weights \u2514\u2500\u2500 last.pt # Last model weight as pytorch format How to run this streamlit webapp project locally? python3 -m venv .venv source .venv/bin/activate git clone https://github.com/Viddesh1/Bike-Helmet-Detectionv2.git cd Bike-Helmet-Detectionv2/ pip install -r local_requirements.txt # For local pip install -r requirements.txt # For deployment streamlit run app.py Note:- If this app is not working locally then please add opencv-python==4.9.0.80 below before opencv-python-headless==4.8.1.78 and opencv-contrib-python==4.8.1.78 in requirements.txt file opencv-python==4.9.0.80 opencv-python-headless==4.8.1.78 opencv-contrib-python==4.8.1.78 Deployment Pipeline Continuous delivery is done by streamlit to host on Streamlit Cloud through this Github repository. Demo Drag and drop the image for object detections Select the video and click Detect Video Objects button Works on only web camera Please make sure web camera is connected Works on native device camera (Webcam, Smartphone) Select respective device and click on start button Insert youtube url and click on Detect Objects button Hosted on Streamlit:- https://bike-helmet-detectionv2-dmehozp3lkef4wnssaepjf.streamlit.app/ Also see 1) https://github.com/Viddesh1/Helmet_test_1 2) https://github.com/Viddesh1/Bike-Helmet-Detection 3) https://github.com/Viddesh1/Bike-Helmet-Detection-Docs","title":"Streamlit"},{"location":"st_bhd/#bike-helmet-detectionv2","text":"This repository contains the Bike Helmet detection using YOLOv8 in streamlit. This web application does processing on end users images and videos to detect bike rider , helmet and *no helmet and render processed images and videos to user.","title":"Bike-Helmet-Detectionv2"},{"location":"st_bhd/#major-python-libraries-used-for-the-project","text":"# Below is the list of the major packages needed for working in this project. ultralytics==8.1.8 # For running inference using YOLOv8 model streamlit==1.30.0 # Web application pillow==10.2.0 # For managing images and videos pytube==15.0.0 # For running inference on small youtube videos","title":"Major python libraries used for the project"},{"location":"st_bhd/#file-structure","text":"Bike-Helmet-Detectionv2 # Root folder of this github repository \u251c\u2500\u2500 app.py # Start page of the application \u251c\u2500\u2500 assets # Necessary assets for testing \u2502 \u251c\u2500\u2500 BikesHelmets6.png \u2502 \u251c\u2500\u2500 video_1.mp4 \u2502 \u251c\u2500\u2500 video_2.mp4 \u2502 \u2514\u2500\u2500 video_3.mp4 \u251c\u2500\u2500 .git # For managing files by git \u251c\u2500\u2500 .gitignore # Files to be not managed by git \u251c\u2500\u2500 helper.py \u251c\u2500\u2500 images # For rendering Predicted and not predicted to streamlit user interface \u2502 \u251c\u2500\u2500 BikesHelmets6_detected.jpg \u2502 \u2514\u2500\u2500 BikesHelmets6.png \u251c\u2500\u2500 local_requirements.txt # For local machine dependencies installation \u251c\u2500\u2500 major_packages.txt # major packages needed for working in this project \u251c\u2500\u2500 packages.txt # Externally insatallion of package by streamlit for deployment \u251c\u2500\u2500 README.md # This readme.md file itself \u251c\u2500\u2500 requirements.txt # For deployment dependencies file \u251c\u2500\u2500 runs \u2502 \u2514\u2500\u2500 detect \u2502 \u2514\u2500\u2500 predict \u2502 \u2514\u2500\u2500 BikesHelmets6.png # Predicted image \u251c\u2500\u2500 settings.py # Configuration blue print for this streamlit web application \u251c\u2500\u2500 videos # For rendering video into streamlit user interface \u2502 \u251c\u2500\u2500 video_1.mp4 \u2502 \u251c\u2500\u2500 video_2.mp4 \u2502 \u2514\u2500\u2500 video_3.mp4 \u2514\u2500\u2500 weights # Model weights \u251c\u2500\u2500 best.pt # Best model weight as pytorch format \u251c\u2500\u2500 information.txt # Information related to placing weights \u2514\u2500\u2500 last.pt # Last model weight as pytorch format","title":"File Structure"},{"location":"st_bhd/#how-to-run-this-streamlit-webapp-project-locally","text":"python3 -m venv .venv source .venv/bin/activate git clone https://github.com/Viddesh1/Bike-Helmet-Detectionv2.git cd Bike-Helmet-Detectionv2/ pip install -r local_requirements.txt # For local pip install -r requirements.txt # For deployment streamlit run app.py Note:- If this app is not working locally then please add opencv-python==4.9.0.80 below before opencv-python-headless==4.8.1.78 and opencv-contrib-python==4.8.1.78 in requirements.txt file opencv-python==4.9.0.80 opencv-python-headless==4.8.1.78 opencv-contrib-python==4.8.1.78","title":"How to run this streamlit webapp project locally?"},{"location":"st_bhd/#deployment-pipeline","text":"Continuous delivery is done by streamlit to host on Streamlit Cloud through this Github repository.","title":"Deployment Pipeline"},{"location":"st_bhd/#demo","text":"","title":"Demo"},{"location":"st_bhd/#drag-and-drop-the-image-for-object-detections","text":"","title":"Drag and drop the image for object detections"},{"location":"st_bhd/#select-the-video-and-click-detect-video-objects-button","text":"","title":"Select the video and click Detect Video Objects button"},{"location":"st_bhd/#works-on-only-web-camera","text":"Please make sure web camera is connected","title":"Works on only web camera"},{"location":"st_bhd/#works-on-native-device-camera-webcam-smartphone","text":"Select respective device and click on start button","title":"Works on native device camera (Webcam, Smartphone)"},{"location":"st_bhd/#insert-youtube-url-and-click-on-detect-objects-button","text":"","title":"Insert youtube url and click on Detect Objects button"},{"location":"st_bhd/#hosted-on-streamlit-","text":"https://bike-helmet-detectionv2-dmehozp3lkef4wnssaepjf.streamlit.app/","title":"Hosted on Streamlit:-"},{"location":"st_bhd/#also-see","text":"1) https://github.com/Viddesh1/Helmet_test_1 2) https://github.com/Viddesh1/Bike-Helmet-Detection 3) https://github.com/Viddesh1/Bike-Helmet-Detection-Docs","title":"Also see"}]}