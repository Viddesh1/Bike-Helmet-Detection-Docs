<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../img/favicon.ico" />
    <title>Index 2 - Bike Rider Helmet Detection Documentation</title>
    <link rel="stylesheet" href="../css/theme.css" />
    <link rel="stylesheet" href="../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Index 2";
        var mkdocs_page_input_path = "index_2.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href=".." class="icon icon-home"> Bike Rider Helmet Detection Documentation
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="..">Index</a>
                </li>
              </ul>
              <ul class="current">
                <li class="toctree-l1 current"><a class="reference internal current" href="#">Index 2</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#computer-vision-introduction">Computer Vision: Introduction</a>
    </li>
    </ul>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../bhd/">BHD</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../dj_bhd/">Django</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../st_bhd/">Streamlit</a>
                </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="..">Bike Rider Helmet Detection Documentation</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href=".." class="icon icon-home" aria-label="Docs"></a></li>
      <li class="breadcrumb-item active">Index 2</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="bike-rider-helmet-detection">Bike Rider Helmet Detection</h1>
<h2 id="computer-vision-introduction">Computer Vision: Introduction</h2>
<p>Computer Vision is an interdisciplinary field of study that enables computers to interpret and understand visual information from the world, much like the human visual system. It encompasses the development of algorithms, models, and systems that can process, analyze, and extract meaningful insights from visual data, typically in the form of images and videos. Computer Vision has wide-ranging applications across various industries, including healthcare, automotive, entertainment, surveillance, robotics, and more. This overview provides a comprehensive understanding of Computer Vision, its key components, applications, challenges, and future prospects.</p>
<h1 id="objective-motivation">Objective / Motivation</h1>
<p>Problem: Bike riders drivers who do not wear helmet which may result in fatal accidents and death in some cases.</p>
<p>Goal: Create a ML/DL model that an detect if a person is wearing helmet or not.</p>
<h1 id="related-work">Related Work</h1>
<p>Key Components of Computer Vision:</p>
<ol>
<li>
<p>Image Acquisition: Computer Vision begins with the acquisition of visual data, which can come from various sources, including cameras, sensors, or image databases.</p>
</li>
<li>
<p>Image Preprocessing: Raw visual data often requires preprocessing to enhance quality, remove noise, and prepare it for analysis. This includes tasks like image resizing, filtering, and color correction.</p>
</li>
<li>
<p>Feature Extraction: Feature extraction involves identifying and isolating relevant visual patterns or features within an image, such as edges, corners, or texture.</p>
</li>
<li>
<p>Object Detection and Recognition: This component focuses on identifying and classifying objects within images or videos. Object detection and recognition algorithms enable computers to recognize and label objects, faces, or specific patterns.</p>
</li>
<li>
<p>Image Segmentation: Image segmentation divides an image into meaningful regions or segments. It's crucial for tasks like medical image analysis, where different parts of an image may represent different anatomical structures.</p>
</li>
<li>
<p>Motion Analysis: Motion analysis techniques track moving objects and can be used in applications like surveillance, sports analysis, and robotics.</p>
</li>
<li>
<p>Scene Understanding: This involves higher-level interpretation of images or videos to understand the context and relationships between objects within a scene.</p>
</li>
</ol>
<h1 id="methodology">Methodology</h1>
<p>1) Data Collection:- Collect publicly availabel images / videos of bike rider, helmet and no helmet for the model to train upon. <br />
2) Data pre-processing:- Pre-processing and autolabel images and videos using foundation model like DINO and SAM (Segment anything model).  <br />
3) Train YOLOv8 Model. <br />
4) Evaluate Target Model.   <br />
5) Run Inference on images and videos.  <br /></p>
<h1 id="tools-and-technologies">Tools and Technologies</h1>
<h2 id="hardware-requirements-">Hardware Requirements :-</h2>
<p>1) Desktop / Laptop / Server <br />
2) 8 GB RAM at least <br />
3) 150 GB Disk space or higher   <br />
4) Any processor Intel i5 / AMD    <br />
5) Google GPU - Tesla T4  <br /></p>
<h2 id="sofware-requirements-">Sofware Requirements :-</h2>
<p>1) Windows / Ubuntu os (64 or 32 bit)   <br /> 
2) Google colab / Kaggle jupyter notebook <br />
3) Python 3.10.12 or higher <br />
4) Visual studio code editor, jupyter notebook <br />
5) Sqlite version 0.5.6 or higher</p>
<h1 id="implementation-details">Implementation details</h1>
<p><img alt="yolov8_metrics" src="../yolov8_images/yolov8_metrics.png" /></p>
<h1 id="what-is-yolov8">What is YOLOv8?</h1>
<p>YOLOv8 is from the YOLO family of models and was released on January 10, 2023. YOLO stands for You Only Look Once, and this series of models are thus named because of their ability to predict every object present in an image with one forward pass. <br /></p>
<h1 id="why-yolov8">Why YOLOv8?</h1>
<p>YOLOv8 by ultalytics is a state-of-the-art deep learning model designed for real-time object detection in computer vision applications. With its advanced architecture and cutting-edge algorithms, YOLOv8 has revolutionized the field of object detection, enabling accurate and efficient detection of objects in real-time scenarios. <br /></p>
<p>YOLOv8 is quite stable as compare to latest YOLOv9 and recent YOLOv10.</p>
<h1 id="dataset-information">Dataset information</h1>
<p><strong>Dataset</strong>:- https://www.kaggle.com/datasets/andrewmvd/helmet-detection <br />
It has 764 images of various Bike rider, Rider wearing helmet, Rider not wearing helmet.</p>
<p>Also, only images are needed for this project and no annotations are needed from the data as annotation are generated by ultralytics framework called as Autodistill which uses Grounding SAM which is combination of Grounding DiNO and SAM (Segment Anything Model) from meta for autolabel dataset and train on YOLOv8. <br /></p>
<p>Autodistill uses big, slower foundation models to train small, faster supervised models. Using autodistill, you can go from unlabeled images to inference on a custom model running at the edge with no human intervention in between. <br /></p>
<p>As foundation models get better and better they will increasingly be able to augment or replace humans in the labeling process.</p>
<p>This project consist of a combination of both the images and videos images. Video is processed is such a way in which every 3 (can be changed through code) frame are considered as data for the model to train upon. <br /></p>
<p>For autolabelling the dataset of images and video frame autodistill uses something which is called as ontology</p>
<p>Ontology - an Ontology defines how your Base Model like Grounding SAM is prompted, what your Dataset will describe, and what your Target Model like YOLOv8 will predict. A simple Ontology is the CaptionOntology which prompts a Base Model with text captions and maps them to class names. Other Ontologies may, for instance, use a CLIP vector or example images instead of a text caption.</p>
<pre><code class="language-python">from autodistill.detection import CaptionOntology

    # &quot;&lt;description of label&gt;&quot;: &quot;&lt;label_name&gt;&quot;
    # &quot;bike rider&quot;: &quot;Bike_Rider&quot;, --&gt; label 0
    # &quot;bike rider and passanger with helmet&quot;: &quot;Helmet&quot;, --&gt; label 1
    # &quot;bike rider and passanger with no helmet&quot;: &quot;No_Helmet&quot; --&gt; label 2

ontology=CaptionOntology({
    &quot;bike rider&quot;: &quot;Bike_Rider&quot;,
    &quot;helmet&quot;: &quot;Helmet&quot;,
    &quot;no helmet&quot;: &quot;No_Helmet&quot;
})
</code></pre>
<p>What are base models? <br />
Base Model - A Base Model is a large foundation model that knows a lot about a lot. Base models are often multimodal and can perform many tasks. They're large, slow, and expensive. Examples of Base Models are GroundedSAM and GPT-4's upcoming multimodal variant. We use a Base Model (along with unlabeled input data and an Ontology) to create a Dataset.  <br /></p>
<p>What are target models? <br />
Target Model - a Target Model is a supervised model that consumes a Dataset and outputs a distilled model that is ready for deployment. Target Models are usually small, fast, and fine-tuned to perform a specific task very well (but they don't generalize well beyond the information described in their Dataset). Examples of Target Models are YOLOv8 and DETR. <br /></p>
<p>How pre-processing of the dataset is done?  <br />
Dataset is a set of auto-labeled data that can be used to train a Target Model. It is the output generated by a Base Model. <br />
The dataset is then divided into training and validation dataset to check the performance of model.</p>
<p>The YOLOv8 model is fine-tuned for custom dataset target values of <strong>bike rider</strong>, <strong>helmet</strong> and <strong>no helmet</strong>. Based on which best.pt and last.pt pytorch training weights are generated. <br /></p>
<p>For more details regarding the jupyter notebook implementation. Take a look at the below github url <br />
https://github.com/Viddesh1/Helmet_test_1 <br /></p>
<p>Fore more details regarding the implementation of the bike helmet detection web application using django. <br />
Take a look at the below github url:- <br />
https://github.com/Viddesh1/Bike-Helmet-Detection <br /></p>
<p>For more details regarding the implementation of the bike helmet detection web application using streamlit. <br />
Take a look at the below github url. <br />
https://github.com/Viddesh1/Bike-Helmet-Detectionv2.git <br /></p>
<h1 id="yolov8-architecture">YOLOv8 Architecture</h1>
<p><strong>YOLOv8 architecture are too big please use Netron.app for more detail and simply open best.pt and last.pt model: https://netron.app/?ref=blog.roboflow.com</strong></p>
<p><img alt="YOLOv8_architecture" src="../yolov8_architecture/YOLOv8_architecture.jpg" /> <br /> <br /> </p>
<h3 id="bestpt-model-architecture">best.pt model architecture</h3>
<p><img alt="bestpt" src="../yolov8_architecture/best.pt.png" />  <br /> <br /> </p>
<h3 id="lastpt-model-architecture">last.pt model architecture</h3>
<p><img alt="lastpt" src="../yolov8_architecture/last.pt.png" />  <br /> <br /> </p>
<h1 id="analysis-of-the-result">Analysis of the result</h1>
<p><img alt="confusion_matrix" src="../yolov8_images/confusion_matrix.png" /> <br /> <br /> 
<img alt="P_curve" src="../yolov8_images/P_curve.png" />   <br /> <br /> 
<img alt="results" src="../yolov8_images/results.png" />   <br /> <br /> </p>
<h1 id="inference-on-images">Inference on images</h1>
<p><img alt="train_batch0" src="../yolov8_images/train_batch0.jpg" /> <br /> <br /> 
<img alt="train_batch1" src="../yolov8_images/train_batch1.jpg" /> <br /> <br /> 
<img alt="train_batch2" src="../yolov8_images/train_batch2.jpg" /> <br /> <br /> </p>
<p><img alt="val_batch0_labels" src="../yolov8_images/val_batch0_labels.jpg" />   <br /> <br /> 
<img alt="val_batch0_pred" src="../yolov8_images/val_batch0_pred.jpg" />   <br /> <br /> </p>
<p><img alt="val_batch1_labels" src="../yolov8_images/val_batch1_labels.jpg" />   <br /> <br /> 
<img alt="val_batch1_pred" src="../yolov8_images/val_batch1_pred.jpg" />   <br /> <br /> </p>
<p><img alt="val_batch2_labels" src="../yolov8_images/val_batch2_labels.jpg" />   <br /> <br /> 
<img alt="val_batch2_pred" src="../yolov8_images/val_batch2_pred.jpg" />   <br /> <br /> </p>
<h1 id="inference-on-video">Inference on Video</h1>
<!-- <iframe width="700" height="500" src="/yolov8_videos/he2.mp4">
</iframe>

<video style="width:50%" muted="" controls="" alt="type:video">
   <source src="yolov8_videos/he2.mp4" type="video/mp4">
</video>

![HE2 Video](/yolov8_videos/he2.mp4)
![Test 1 Video](/yolov8_videos/test_1.mp4)
![Test 2 Video](/yolov8_videos/test_2.mp4) 

<iframe width="700" height="500" src="/yolov8_videos/test_1.mp4">
</iframe>

<iframe width="700" height="500" src="/yolov8_videos/test_2.mp4">
</iframe> -->

<p><img alt="All_Video_Pred" src="../yolov8_images/All_Video_Pred.png" /> <br /> <br /> </p>
<h1 id="conclusion">Conclusion</h1>
<p>In conclusion, the development of a computer vision model using YOLOv8 for bike rider helmet detection represents a significant advancement in enhancing safety measures for riders. The model demonstrates impressive accuracy and efficiency in identifying helmet-wearing individuals, which is crucial for ensuring compliance with safety regulations and reducing the risk of head injuries in bike-related accidents.</p>
<p>This project highlights the potential of computer vision technology to address real-world safety challenges effectively. </p>
<h1 id="future-enhancement">Future enhancement</h1>
<p>YOLOv9 implementation <br />
YOLOv10 implementation  <br /></p>
<h1 id="program-code">Program code</h1>
<p>1) Implementation of Bike Helmet Detection Jupyter Notebook <br />
https://github.com/Viddesh1/Helmet_test_1   <br /></p>
<p>2) Output generated by YOLOv8   <br />
https://drive.google.com/drive/folders/1M4FckJJeyPQTTWqgo6xWhW8L4tf0EJ4l?usp=sharing   <br /></p>
<p>3) Bike Helmet Detection Django Web application <br />
https://github.com/Viddesh1/Bike-Helmet-Detection   <br /></p>
<p>4) Bike Helmet Detection Stremlit Web application   <br />
https://github.com/Viddesh1/Bike-Helmet-Detectionv2 <br /></p>
<p>5) Hosted on Streamlit  <br />
https://bike-helmet-detectionv2-dmehozp3lkef4wnssaepjf.streamlit.app/   <br /></p>
<p>6) Overall Documentation    <br />
https://github.com/Viddesh1/Bike-Helmet-Detection-Docs  <br /></p>
<h1 id="references">References</h1>
<p>1) https://github.com/ultralytics/ultralytics</p>
<p>2) https://towardsdatascience.com/dino-emerging-properties-in-self-supervised-vision-transformers-summary-ab91df82cc3c</p>
<p>3) https://github.com/facebookresearch/dino</p>
<p>4) Emerging Properties in Self-Supervised Vision Transformers :- https://arxiv.org/abs/2104.14294</p>
<p>5) https://dinov2.metademolab.com/</p>
<p>6) https://segment-anything.com/</p>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href=".." class="btn btn-neutral float-left" title="Index"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../bhd/" class="btn btn-neutral float-right" title="BHD">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href=".." style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../bhd/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "..";</script>
    <script src="../js/theme_extra.js"></script>
    <script src="../js/theme.js"></script>
      <script src="../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
