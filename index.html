<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="description" content="None" />
      <link rel="shortcut icon" href="img/favicon.ico" />
    <title>Bike Rider Helmet Detection Documentation</title>
    <link rel="stylesheet" href="css/theme.css" />
    <link rel="stylesheet" href="css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Index";
        var mkdocs_page_input_path = "index.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="." class="icon icon-home"> Bike Rider Helmet Detection Documentation
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="./search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
                <li class="toctree-l1 current"><a class="reference internal current" href="#">Index</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#abstract">Abstract</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#index">Index</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#chapter-1">Chapter 1</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#1introduction">1.Introduction</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#chapter-2">Chapter 2</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#2-objective">2. Objective</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#chapter-3">Chapter 3</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#3-tools-and-technology">3. Tools and Technology</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#chapter-4">Chapter 4</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#4-literature-review">4. Literature Review</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#chapter-5">Chapter 5</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#5-methodology">5. Methodology</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#chapter-6">Chapter 6</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#6-implementation-details">6. Implementation details</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#chapter-7">Chapter 7</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#7-analysis-of-result">7. Analysis of result</a>
    </li>
    </ul>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="index_2/">Index 2</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="bhd/">BHD</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="dj_bhd/">Django</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="st_bhd/">Streamlit</a>
                </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href=".">Bike Rider Helmet Detection Documentation</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="." class="icon icon-home" aria-label="Docs"></a></li>
      <li class="breadcrumb-item active">Index</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="bike-rider-helmet-detection">Bike Rider Helmet Detection</h1>
<h2 id="abstract">Abstract</h2>
<p>Bike rider helmet detection is a crucial computer vision task aimed at improving
road safety by identifying bike rider who are not wearing helmets. This system
leverages deep learning models, such as YOLOv8, to accurately detect helmets
in real-time or from images or video streams. It can be integrated into traffic
monitoring systems to automatically flag violations and enhance law enforcement
efficiency. The implementation involves preprocessing images, training the model,
and saving the model. I have also created a streamlit and django web application
and deployed the web application in streamlit cloud. The proposed system helps
reduce head injuries and fatalities in bike rider accidents by promoting helmet use. <br /></p>
<hr />
<h2 id="index">Index</h2>
<table>
<thead>
<tr>
<th style="text-align: center;">Sr. No</th>
<th style="text-align: left;">Topic</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: left;">1) Introduction</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: left;">1.1) Computer Vision: Introduction</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: left;">1.2) Bike Rider Helmet Detection: Introduction</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: left;">2) Objective</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: left;">2.1) Problem</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: left;">2.2) Goal</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: left;">3) Tools and Technology</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: left;">3.1) Hardware Requirements</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: left;">3.2) Software Requirements</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: left;">4) Literature Review</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: left;">4.1) Image Acquisition</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: left;">4.2) Image Preprocessing</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: left;">4.3) Feature Extraction</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: left;">4.4) Object Detection and Recognition</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: left;">4.5) Image Segmentation</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: left;">4.6) Motion Analysis</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: left;">4.7) Scene Understanding</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: left;">5) Methodology</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: left;">5.1) Data Collection</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: left;">5.2) Data Pre-processing</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: left;">5.3) Model Training</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: left;">5.4) Model Evaluation</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: left;">5.5) Model Testing</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: left;">5.6) Deployment</td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: left;">6) Implementation Details</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: left;">6.1) Dataset Used</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: left;">6.2) YOLOv8 Metrics</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: left;">6.3) What is YOLOv8?</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: left;">6.4) Why YOLOv8?</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: left;">6.5) What is Ultralytics?</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: left;">6.6) What are Base Models?</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: left;">6.7) What are Target Models?</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: left;">6.8) How Pre-processing of the Dataset is Done?</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: left;">6.9) How Images are Pre-processed?</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: left;">6.10) How Videos are Pre-processed?</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: left;">6.11) What is Ontology?</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: left;">6.12) Sample Code</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: left;">6.13) Data and YOLOv8 Configuration File</td>
</tr>
<tr>
<td style="text-align: center;">7</td>
<td style="text-align: left;">7) Analysis of Result</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: left;">7.1) Confusion Matrix</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: left;">7.2) Precision-Recall Curve and Train, Validation, and Loss Metrics</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: left;">7.3) F1-Confidence and Recall-Confidence Curve</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: left;">7.4) Precision-Recall Curve</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: left;">7.5) Labels-Class Chart</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: left;">7.6) Inference on Images</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: left;">7.7) Inference on Videos</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: left;">7.8) Django Web Application Demo</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: left;">7.9) Streamlit Web Application Demo</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: left;">7.10) Mkdocs Documentation Demo</td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: left;">8) Conclusion</td>
</tr>
<tr>
<td style="text-align: center;">9</td>
<td style="text-align: left;">9) Future Scope</td>
</tr>
<tr>
<td style="text-align: center;">10</td>
<td style="text-align: left;">10) Program Code</td>
</tr>
<tr>
<td style="text-align: center;">11</td>
<td style="text-align: left;">11) Reference</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="chapter-1">Chapter 1</h2>
<h2 id="1introduction">1.Introduction</h2>
<p><strong>1.1) Computer Vison: Introduction</strong></p>
<p>Computer Vision is an interdisciplinary field of study that enables computers to
interpret and understand visual information from the world, much like the human
visual system. It encompasses the development of algorithms, models, and systems
that can process, analyze, and extract meaningful insights from visual data, typically
in the form of images and videos. Computer Vision has wide-ranging applications
across various industries, including healthcare, automotive, entertainment,
surveillance, robotics, and more. This overview provides a comprehensive
understanding of Computer Vision, its key components, applications, challenges,
and future prospects. <br /></p>
<p><strong>1.2) Bike Rider Helmet Detection: Introduction</strong></p>
<p>The importance of road safety has been increasingly recognized, leading to the
implementation of various measures to protect vulnerable road users such as cyclists
and motorcyclists or bike riders. One critical safety measure is the use of helmets,
which significantly reduces the risk of head injuries in the event of an accident.
Despite regulations mandating helmet use, compliance remains a challenge. To
address this, the Bike Rider Helmet Detection project aims to leverage advanced
computer vision techniques to automatically detect helmet usage among bike riders
and passangers. <br /></p>
<p>The Bike Rider Helmet Detection project utilizes one of the state-of-the-art deep
learning models to accurately identify whether a bike rider is wearing a helmet. The
core of the project is built on the YOLOv8 (You Only Look Once version 8) model,
a highly efficient and powerful object detection algorithm developed and maintained
by Ultralytics. YOLOv8 is renowned for its speed and accuracy, making it an ideal
choice for real-time applications like helmet detection. <br /></p>
<p>There are a significant number of the accidents on the roads today. The accident can
have multiple scenarios whether due to pot holes or any car accidentally bumbs bike
driver while driving etc. By wearing a helmet not only we are saving our head from
any fatal accidents but also saving any passanger life too. <br /></p>
<p>This explains why it is important to do more work in this field with an aim to reduce
the occurrence of accidents related to any driving related injury and motivate
overselves the importance of a helmet while riding a 2 wheeler vehicle. <br /></p>
<hr />
<h2 id="chapter-2">Chapter 2</h2>
<h2 id="2-objective">2. Objective</h2>
<p><strong>2.1) Problem:</strong></p>
<p>Bike riders drivers who do not wear helmet which may result in fatal accidents and
death in some cases. <br /></p>
<p><strong>2.2) Goal:</strong></p>
<p>Create a deep learning model that an detect if a person is wearing helmet or not. <br /></p>
<p>The main objective of this project is to develop a system that would detect if the
bike rider along with the passengers are wearing a safety helmet or not. The dataset
is collected through kaggle dataset. As manually annotation of the labels or semi
label annotation would take significant amount of time. Only images are needed and
and no explicit annotations are needed for this project as I am using ultralytics
foundation models for auto labelling the images and video images to target model of
YOLOv8. After training the model I have made a streamlit and django web
application for this project along with documentation. <br /></p>
<hr />
<h2 id="chapter-3">Chapter 3</h2>
<h2 id="3-tools-and-technology">3. Tools and Technology</h2>
<p>As this is a deep learning project a significant amount of computation and memory
allocation does matter a lot for this project. For inference of model on images and
videos too required a significant amount of computation power. <br /></p>
<p><strong>3.1) Hardware Requirements :-</strong> <br /></p>
<p>1) Desktop / Laptop / Server    <br />
2) 8 GB RAM at least    <br />
3) 150 GB Disk space or higher  <br />
4) Any processor Intel i5 / AMD <br />
5) Google GPU - Tesla T4    <br /></p>
<p><strong>3.2) Sofware Requirements :-</strong>    <br /></p>
<p>1) Windows / Ubuntu os (64 or 32 bit)   <br />
2) Google colab / Kaggle jupyter notebook   <br />
3) Python 3.10.12 or higher <br />
4) Visual studio code editor, jupyter notebook  <br />
5) Sqlite version 0.5.6 or higher   <br /></p>
<p>For more in depth requirements of the major, minor packages and respective project
dependencies. Please take a look at the respective github repository. <br /></p>
<hr />
<h2 id="chapter-4">Chapter 4</h2>
<h2 id="4-literature-review">4. Literature Review</h2>
<p><strong>Key Components of Computer Vision:</strong> <br /></p>
<p><strong>4.1) Image Acquisition:</strong></p>
<p>Computer Vision begins with the acquisition of visual data, which can come from
various sources, including cameras, sensors, or image databases.</p>
<p><strong>4.2) Image Preprocessing:</strong></p>
<p>Raw visual data often requires preprocessing to enhance quality, remove noise, and
prepare it for analysis. This includes tasks like image resizing, filtering, and color
correction.</p>
<p><strong>4.3) Feature Extraction:</strong></p>
<p>Feature extraction involves identifying and isolating relevant visual patterns or
features within an image, such as edges, corners, or texture.</p>
<p><strong>4.4) Object Detection and Recognition:</strong></p>
<p>This component focuses on identifying and classifying objects within images or
videos. Object detection and recognition algorithms enable computers to recognize
and label objects, faces, or specific patterns.</p>
<p><strong>4.5) Image Segmentation:</strong></p>
<p>Image segmentation divides an image into meaningful regions or segments. It's
crucial for tasks like medical image analysis, where different parts of an image may
represent different anatomical structures.</p>
<p><strong>4.6) Motion Analysis:</strong></p>
<p>Motion analysis techniques track moving objects and can be used in applications
like surveillance, sports analysis, and robotics.</p>
<p><strong>4.7) Scene Understanding:</strong></p>
<p>This involves higher-level interpretation of images or videos to understand the
context and relationships between objects within a scene.</p>
<hr />
<h2 id="chapter-5">Chapter 5</h2>
<h2 id="5-methodology">5. Methodology</h2>
<p><strong>5.1) Data Collection:</strong></p>
<p>Collect publicly available images / videos of bike rider, helmet and no helmet for the
model to train upon. The images are collected from kaggle dataset</p>
<p><strong>5.2) Data pre-processing:</strong></p>
<p>Pre-processing and auto labeling images and videos are done by ultralytics
framework by using a Grounding Segment Anything Model based on Ontology. For
videos every 3 frame is considered.</p>
<p><strong>5.3) Model Training:</strong></p>
<p>Train YOLOv8 Model on custom dataset with 3 classes, Socastic Gradient Decent
(SGD) optimizer, learning rate 0.01, batch size 16, epochs 50, image size 640,
momentum 0.937, validation split around 20 %.</p>
<p><strong>5.4) Model Evaluation:</strong></p>
<p>Evaluate trained YOLOv8 Model on validation dataset with the metrics such as box
loss, class loss, precision and recall. Confusion matrix.</p>
<p><strong>5.5) Model Testing:</strong></p>
<p>Run Inference on images and videos. To check how well the model perform.</p>
<p><strong>5.6) Deployment:</strong></p>
<p>The final model is deployed as a streamlit web application and django for local
deployment along with application programming interface (API).</p>
<hr />
<h2 id="chapter-6">Chapter 6</h2>
<h2 id="6-implementation-details">6. Implementation details</h2>
<p><strong>6.1) Dataset Used:</strong></p>
<p>The dataset that is used for this project can be find below:-</p>
<p>https://www.kaggle.com/datasets/andrewmvd/helmet-detection</p>
<p>This dataset contains 764 images of 2 distinct classes for the objective of helmet
detection.</p>
<p>Bounding box annotations are provided in the PASCAL VOC format. <br /></p>
<p>The classes are:    <br />
- With helmet   <br />
- Without helmet    <br /></p>
<p><strong>6.2) YOLOv8 Metrics:</strong></p>
<p><img alt="yolov8_metrics" src="yolov8_images/yolov8_metrics.png" /></p>
<p><strong>6.3) What is YOLOv8?</strong></p>
<p><strong>YOLOv8</strong> is from the YOLO family of models and was released on January 10,
2023. YOLO stands for You Only Look Once, and this series of models are thus
named because of their ability to predict every object present in an image with one
forward pass.</p>
<p><strong>6.4) Why YOLOv8?</strong></p>
<p><strong>YOLOv8</strong> by ultalytics is a state-of-the-art deep learning model designed for real-
time object detection in computer vision applications. With its advanced architecture
and cutting-edge algorithms, YOLOv8 has revolutionized the field of object
detection, enabling accurate and efficient detection of objects in real-time scenarios.</p>
<p>YOLOv8 is quite stable as compare to latest YOLOv9 and recent YOLOv10.</p>
<p><strong>6.5) What is Ultralytics?</strong></p>
<p><strong>Ultralytics</strong> is a company that specializes in developing advanced computer vision
technologies, particularly the YOLO (You Only Look Once) series of models. Their
framework, Ultralytics YOLO, provides state-of-the-art object detection, image
segmentation, and classification capabilities. The base model, often a pre-trained
YOLOv8 model or it can be any model like YOLOv5, can be fine-tuned or used
directly to create custom target models for various applications for any custom
dataset. This framework is widely used for real-time object detection tasks due to its
speed and accuracy.</p>
<p><strong>6.6) What are base models?</strong></p>
<p><strong>Base Model</strong> - A Base Model is a large foundation model that knows a lot about a
lot. Base models are often multimodal and can perform many tasks. They're large,
slow, and expensive. Examples of Base Models are GroundedSAM and GPT-4's
upcoming multimodal variant. We use a Base Model (along with unlabeled input
data and an Ontology) to create a Dataset.</p>
<p><strong>6.7) What are target models?</strong></p>
<p><strong>Target Model</strong> - a Target Model is a supervised model that consumes a dataset and
outputs a distilled model that is ready for deployment. Target Models are usually
small, fast, and fine-tuned to perform a specific task very well (but they don't
generalize well beyond the information described in their Dataset). Examples of
Target Models are YOLOv8 and YOLOv5.</p>
<p><strong>6.8) How pre-processing of the dataset is done?</strong></p>
<p>Only images and videos are needed for this project and no annotations are needed
from the data as annotation are generated by ultralytics framework called as
<strong>Autodistill</strong> which uses Grounding SAM which is combination of Grounding
DiNO (Zero short object detection model) and SAM (Segment Anything Model)
(zero short object detection with prompting) from Meta for autolabeling dataset
and preprocess and train on YOLOv8 large model.</p>
<p>Autodistill uses big, slower foundation models to train small, faster supervised
models. Using autodistill, you can go from unlabeled images to inference on a
custom model running at the edge with no human intervention in between.</p>
<p>As foundation models get better and better they will increasingly be able to augment
or replace humans in the labeling process.</p>
<p><strong>6.9) How images are pre-processed?</strong></p>
<p>The dataset image consist of 764 images of various Bike rider, Rider wearing
helmet, Rider not wearing helmet. Out of 764 images only 611 images are used. I
have removed some images because my google colab kernel is crashing
significantly.</p>
<p><strong>6.10) How videos are pre-processed?</strong></p>
<p>Video is processed is such a way in which every 3 frame (can be changed through
code) are considered as image data for the model to train upon.</p>
<p>For autolabelling the dataset of images and video frame autodistill uses something
which is called as ontology</p>
<p><strong>6.11) What is Ontology?</strong></p>
<p><strong>Ontology</strong> - an Ontology defines how your Base Model like Grounding SAM is
prompted, what your Dataset will describe, and what your Target Model like
YOLOv8 will predict. A simple Ontology is the CaptionOntology which prompts a
Base Model with text captions and maps them to class names. Other Ontologies
may, for instance, use a CLIP vector or example images instead of a text caption.</p>
<pre><code class="language-python">from autodistill.detection import CaptionOntology

# &quot;&lt;description of label&gt;&quot;: &quot;&lt;label_name&gt;&quot;
# &quot;bike rider&quot;: &quot;Bike_Rider&quot;, --&gt; label 0
# &quot;bike rider and passanger with helmet&quot;: &quot;Helmet&quot;, --&gt; label 1
# &quot;bike rider and passanger with no helmet&quot;: &quot;No_Helmet&quot; --&gt; label 2

ontology=CaptionOntology({
&quot;bike rider&quot;: &quot;Bike_Rider&quot;,
&quot;helmet&quot;: &quot;Helmet&quot;,
&quot;no helmet&quot;: &quot;No_Helmet&quot;
})
</code></pre>
<p>The preprocessed dataset is then divided into training and validation dataset to check
the performance of model.</p>
<p>This YOLOv8 model is fine-tuned for custom dataset target values of <strong>bike rider</strong>,
<strong>helmet</strong> and <strong>no helmet</strong>. Based on which best.pt and last.pt pytorch training weights
are generated.</p>
<p><strong>6.12) Sample Code</strong></p>
<p><img alt="sample_code_1" src="Jupyter_notebook_op/sample_code_1.png" /></p>
<p><img alt="sample_code_2" src="Jupyter_notebook_op/sample_code_2.png" /></p>
<p><img alt="sample_code_3" src="Jupyter_notebook_op/sample_code_3.png" /></p>
<p><img alt="sample_code_4" src="Jupyter_notebook_op/sample_code_4.png" /></p>
<p><img alt="sample_code_5" src="Jupyter_notebook_op/sample_code_5.png" /></p>
<p><img alt="sample_op_6" src="Jupyter_notebook_op/sample_code_6.png" /></p>
<p><img alt="sample_code_7" src="Jupyter_notebook_op/sample_code_7.png" /></p>
<p><img alt="sample_code_8" src="Jupyter_notebook_op/sample_code_8.png" /></p>
<p><img alt="sample_code_9" src="Jupyter_notebook_op/sample_code_9.png" /></p>
<p><img alt="sample_code_10" src="Jupyter_notebook_op/sample_code_10.png" /></p>
<p><strong>6.13) Data and YOLOv8 configuration file</strong></p>
<p><img alt="sample_code_11" src="Jupyter_notebook_op/sample_code_11.png" /></p>
<p><img alt="sample_code_12" src="Jupyter_notebook_op/sample_code_12.png" /></p>
<hr />
<h2 id="chapter-7">Chapter 7</h2>
<h2 id="7-analysis-of-result">7. Analysis of result</h2>
<p><strong>7.1) Confusion Matrix</strong></p>
<p><img alt="confusion_matrix" src="yolov8_images/confusion_matrix.png" /> <br /> <br /> </p>
<p><strong>7.2) Precision-Confidence Curve and Train, Validation and Loss metrics</strong></p>
<p><img alt="P_curve" src="yolov8_images/P_curve.png" />   <br /> <br /> </p>
<p><img alt="results" src="yolov8_images/results.png" />   <br /> <br /> </p>
<p><strong>7.3) F1-Confidence and Recall-Confidence Curve</strong></p>
<p><img alt="F1_curve" src="yolov8_images/F1_curve.png" />   <br /> <br /> 
<img alt="R_curve" src="yolov8_images/R_curve.png" />   <br /> <br /> </p>
<p><strong>7.4) Precision-Recall Curve</strong></p>
<p><img alt="PR_curve" src="yolov8_images/PR_curve.png" />   <br /> <br /> 
<img alt="labels_correlogram" src="yolov8_images/labels_correlogram.jpg" />   <br /> <br /> </p>
<p><strong>7.5) Labels-Class Chart</strong></p>
<p><img alt="labels" src="yolov8_images/labels.jpg" />   <br /> <br /> </p>
<p><strong>7.6) Inference on Images</strong></p>
<p><img alt="train_batch0" src="yolov8_images/train_batch0.jpg" /> <br /> <br /> 
<img alt="train_batch1" src="yolov8_images/train_batch1.jpg" /> <br /> <br /> 
<img alt="train_batch2" src="yolov8_images/train_batch2.jpg" /> <br /> <br /> </p>
<p><img alt="val_batch0_labels" src="yolov8_images/val_batch0_labels.jpg" />   <br /> <br /> 
<img alt="val_batch0_pred" src="yolov8_images/val_batch0_pred.jpg" />   <br /> <br /> </p>
<p><img alt="val_batch1_labels" src="yolov8_images/val_batch1_labels.jpg" />   <br /> <br /> 
<img alt="val_batch1_pred" src="yolov8_images/val_batch1_pred.jpg" />   <br /> <br /> </p>
<p><img alt="val_batch2_labels" src="yolov8_images/val_batch2_labels.jpg" />   <br /> <br /> 
<img alt="val_batch2_pred" src="yolov8_images/val_batch2_pred.jpg" />   <br /> <br /> </p>
<h1 id="inference-on-video">Inference on Video</h1>
<p><img alt="All_Video_Pred" src="yolov8_images/All_Video_Pred.png" /> <br /> <br /> </p>
<p><strong>7.8) Django Web application Demo</strong></p>
<h3 id="index-page">Index Page</h3>
<p><img alt="Screenshot_1" src="dj_bhd_images/Screenshot_1.png" /> <br /> <br /> 
<img alt="Screenshot_2" src="dj_bhd_images/Screenshot_2.png" /> <br /> <br /></p>
<h3 id="django-admin-panel">Django admin panel</h3>
<p><img alt="Screenshot_3" src="dj_bhd_images/Screenshot_3.png" /> <br /> <br />
<img alt="Screenshot_4" src="dj_bhd_images/Screenshot_4.png" /> <br /> <br /></p>
<h3 id="sqlite3-database">Sqlite3 database</h3>
<p><img alt="ml_app_image" src="dj_bhd_images/ml_app_image.png" /> <br /> <br />
<img alt="ml_app_predimage" src="dj_bhd_images/ml_app_predimage.png" /> <br /> <br /></p>
<h3 id="api-end-points">API end points</h3>
<p><img alt="Screenshot_5" src="dj_bhd_images/Screenshot_5.png" /> <br /> <br />
<img alt="Screenshot_6" src="dj_bhd_images/Screenshot_6.png" /> <br /> <br /></p>
<p><strong>7.9) Streamlit Web application Demo</strong></p>
<h2 id="drag-and-drop-the-image-for-object-detections">Drag and drop the image for object detections</h2>
<p><img alt="st_image_pred" src="st_bhd_images/st_image_pred.png" /></p>
<h2 id="select-the-video-and-click-detect-video-objects-button">Select the video and click Detect Video Objects button</h2>
<p><img alt="st_video_pred" src="st_bhd_images/st_video_pred.png" /></p>
<h2 id="works-on-only-web-camera">Works on only web camera</h2>
<p><strong>Please make sure web camera is connected</strong></p>
<p><img alt="st_webcam_pred" src="st_bhd_images/st_webcam_pred.png" /></p>
<h2 id="works-on-native-device-camera-webcam-smartphone">Works on native device camera (Webcam, Smartphone)</h2>
<p><strong>Select respective device and click on start button</strong></p>
<p><img alt="st_devicecam_pred" src="st_bhd_images/st_devicecam_pred.png" /></p>
<h2 id="insert-youtube-url-and-click-on-detect-objects-button">Insert youtube url and click on Detect Objects button</h2>
<p><img alt="st_yt_pred" src="st_bhd_images/st_yt_pred.png" /></p>
<p><strong>7.10) Mkdocs Documentation Demo</strong></p>
<p><img alt="st_yt_pred" src="docs_images/docs_index.png" /> <br /><br />
<img alt="st_yt_pred" src="docs_images/docs_bhd.png" /> <br /><br /></p>
<hr />
<h2 id="chapter-8">Chapter 8</h2>
<h2 id="8-conclusion">8. Conclusion</h2>
<p>In conclusion, the development of a computer vision model using YOLOv8 for bike
rider helmet detection represents a significant advancement in enhancing safety
measures for riders along with passengers. The model demonstrates impressive
accuracy and efficiency in identifying helmet-wearing individuals, which is crucial
for ensuring compliance with safety regulations and reducing the risk of head
injuries in bike-related accidents.</p>
<p>This project highlights the potential of computer vision technology to address real-
world safety challenges effectively.</p>
<hr />
<h2 id="chapter-9">Chapter 9</h2>
<h2 id="9-future-scope">9. Future Scope</h2>
<p>The Future scope of this project should be focused on latest implementation of
YOLOv9 and recent YOLOv10. There should be seperate repository for
implementation of YOLOv9 and YOLOv10 along with any demo related to it with
any technology for creating web application like django, flask, mesop, streamlit,
gradio etc. or any desktop application using various techology as reflex, nicegui,
tkinter, kivy etc.</p>
<hr />
<h2 id="chapter-10">Chapter 10</h2>
<h2 id="10-program-code">10. Program code</h2>
<p>1) Implementation of Bike Helmet Detection Jupyter Notebook
https://github.com/Viddesh1/Helmet_test_1</p>
<p>2) Output generated by YOLOv8
https://drive.google.com/drive/folders/
1M4FckJJeyPQTTWqgo6xWhW8L4tf0EJ4l?usp=sharing</p>
<p>3) Bike Helmet Detection Django Web application
https://github.com/Viddesh1/Bike-Helmet-Detection</p>
<p>4) Bike Helmet Detection Stremlit Web application
https://github.com/Viddesh1/Bike-Helmet-Detectionv2</p>
<p>5) Hosted on Streamlit
https://bike-helmet-detectionv2-dmehozp3lkef4wnssaepjf.streamlit.app/</p>
<p>6) Overall Documentation
https://github.com/Viddesh1/Bike-Helmet-Detection-Docs</p>
<hr />
<h2 id="chapter-11">Chapter 11</h2>
<h2 id="11-references">11. References</h2>
<p>1) https://github.com/ultralytics/ultralytics</p>
<p>2) https://towardsdatascience.com/dino-emerging-properties-in-self-supervised-vision-
transformers-summary-ab91df82cc3c</p>
<p>3) https://github.com/facebookresearch/dino/</p>
<p>4) Emerging Properties in Self-Supervised Vision Transformers :
https://arxiv.org/abs/2104.14294/</p>
<p>5) https://dinov2.metademolab.com/</p>
<p>6) https://segment-anything.com/</p>
<p>7) https://blog.roboflow.com/whats-new-in-yolov8/</p>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="index_2/" class="btn btn-neutral float-right" title="Index 2">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
    
      <span><a href="index_2/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="js/jquery-3.6.0.min.js"></script>
    <script>var base_url = ".";</script>
    <script src="js/theme_extra.js"></script>
    <script src="js/theme.js"></script>
      <script src="search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>

<!--
MkDocs version : 1.6.0
Build Date UTC : 2024-07-06 02:03:38.125105+00:00
-->
